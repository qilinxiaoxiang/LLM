## 1.  What is vision models?
1. 这里主要讲的是文生图模型.
2. 其他视觉相关模型
   1. 分类模型：将输入图像分为不同的类别。
   2. 目标检测模型：检测图像中的目标。
   3. 分割模型：将图像分割成不同的区域。
   4. 识别模型：识别图像中的物体。
   5. 检索模型：检索相似的图像。
   6. 重建模型：重建图像, 尤其是通过2D图像重建3D场景的模型.

## 2. Why do we need vision models?
1. 图像处理是人类的基本技能，但是对于计算机来说，图像处理是一项复杂的任务。
2. 如果计算机能够准确识别图像中的物体，那么它就可以更好地理解世界, 也可能更好地和世界交互。
3. 图像绘制原本是需要人工完成的，但是现在可以通过计算机模型来实现, 那么可以帮助不会画图的人拓展创作的可能性。

## 3. How to Implement vision models?
### 3.1 2D图像生成
1. **自编码器（Autoencoder）**：
   - **诞生原因与问题解决：** 自编码器最早用于无监督学习和降维。它们被设计用于学习数据的紧凑表示形式，其中包括输入数据和尽可能相似的输出数据。
   - **实现方式：** 自编码器包括编码器和解码器两部分。编码器将输入数据编码为潜在空间中的表示，解码器则将这种表示解码为重构的输入数据。
   - **缺点与克服：** 传统自编码器容易受到过拟合的影响，并且不能很好地处理潜在空间中的连续变化。后续的变体模型如 VAE 和 Diffusion Model 试图解决这些问题。
2. **变分自编码器（Variational Autoencoder，VAE）**：
   - **诞生原因与问题解决：** VAE的提出是为了解决传统自编码器在生成样本时的缺陷，尤其是在对潜在空间进行连续插值时的效果不佳。
   - **实现方式：** VAE引入了概率分布来表示潜在空间的编码，使得生成样本时可以从分布中采样，从而使得样本之间的插值更加平滑。
   - **缺点与克服：** VAE的主要缺点之一是生成的样本往往比较模糊，这是由于其损失函数中的重建项和正则化项之间的权衡。后来的模型如 Diffusion Model 和 Stable Diffusion Model 尝试改进生成图像的质量。
       - 在VAE中，损失函数通常由两部分组成：重建损失（reconstruction loss）和正则化项（regularization term）。重建损失用于衡量原始输入图像和生成图像之间的差异，而正则化项用于限制潜在空间的复杂度，避免过拟合。 但是，重建损失和正则化项之间的权衡可能导致生成的图像模糊，因为模型可能会过度关注正则化项，而忽略重建损失。这可能导致生成的图像缺乏清晰度和细节。
3. **扩散模型（Diffusion Model）**：
   - **诞生原因与问题解决：** Diffusion Model 旨在提高生成图像的质量，尤其是在处理高分辨率图像时。
   - **实现方式：** Diffusion Model 采用了一种逐步增加噪声的方式来生成图像，避免了直接对重建损失和正则化项进行权衡。这种逐步增加噪声的过程可以帮助模型更好地捕获数据分布的细节，从而生成更加清晰的图像，而不受正则化项的影响, 从而使得生成的图像逐渐逼近真实分布。
       - 基于UNet
           - UNet是一种卷积神经网络结构(CNN)，由编码器和解码器组成，并且在解码器中使用了跳跃连接，这些跳跃连接帮助将编码器中的低级特征与解码器中的高级特征相结合，从而更好地保留图像的空间信息，有助于改善图像分割的精度。
       - 基于Transformer
           - Transformer是一种基于自注意力机制的神经网络结构，主要由多头自注意力层和全连接前馈网络组成，没有卷积结构。
           - Transformer模型的特点是能够并行计算，使得在处理长序列数据时具有较好的性能。
   - **缺点与克服：** Diffusion Model 在生成高分辨率图像时需要大量的计算资源和时间，同时对于长期依赖的建模效果也不尽如人意。这些问题促使了 Stable Diffusion Model 的提出。
       - "长期依赖"指的是生成图像时需要考虑的像素之间的远距离相关性或依赖关系。
       - 在图像生成中，长期依赖可能指的是图像中不同区域之间的复杂相关性，例如对象的局部和全局结构之间的关系。
4. **稳定扩散模型（Stable Diffusion Model）**：
   - **诞生原因与问题解决：** Stable Diffusion Model 的目标是提高 Diffusion Model 在生成高质量图像时的稳定性和效率。
   - **实现方式：** Stable Diffusion Model 引入了更稳定的训练方法，同时通过改进建模长期依赖的方式来提高生成图像的质量。
   - **缺点与克服：** 尽管 Stable Diffusion Model 改进了训练的稳定性和效率，但仍然存在一些挑战，例如对于复杂数据分布的建模能力有限。
5. **级联式文生图模型**：
   - **诞生原因与问题解决：** 级联式文生图模型是为了更好地处理生成图像时的长期依赖关系，以及更好地建模复杂的数据分布。
   - **实现方式：** 该模型采用了级联式的结构来处理长期依赖，也就是先生成一个低像素的图像, 然后再逐渐丰富成高像素. 同时引入了更复杂的网络结构来提高建模能力。
   - **缺点与克服：** 这个模型还在发展中，一些挑战可能包括训练的复杂性和计算资源的需求。

#### Stable Diffusion 模型详解

Stable Diffusion模型是一种基于扩散过程的图像生成模型，它结合了UNet结构和Transformer的注意力机制，以实现高质量的图像生成。这个模型的工作原理基于逐步去除噪声的过程，将图像从纯噪声状态转换成清晰图像。

##### 主要组件

1. **Pixel Space**:
   - 包括原始图像 `x` 和去噪后的图像 `x̃`。
   - 经过解码器 `D` 生成去噪图像。

2. **Latent Space**:
   - 扩散过程中的潜在空间，包含中间态的图像表示。

3. **Denoising U-Net**:
   - U-Net结构被用于从带噪声的图像逐步恢复出清晰图像。
   - 包含多个跨注意力层（cross-attention layers），这些层通过 `Q`（Query）和 `KV`（Key-Value）的方式实现特征的提取和融合。

4. **Conditioning Techniques**:
   - 使用语义映射、文本描述或其他图像表示作为条件，指导生成过程以符合特定的主题或风格。

##### 版本演进

- **SD v1.4 和 SD v1.5**:
  - 使用CLIP作为语义理解的驱动，结合CNN构成的UNet，主要面向512x512分辨率的图像生成。
  - SD v1.5优化了对长尾分布特征的处理，提高了生成质量。

- **SD v2.1**:
  - 提高了解析度至768x768，使用Open-CLIP改善文本到图像的映射质量。

- **SD XL**:
  - 结合CLIP和OpenCLIP，进一步扩展至1024x1024的高分辨率图像生成。
  - 针对大型数据集和更复杂的图像内容优化。

- **PixelArt**:
  - 特别版本，采用T5语言模型和ViT图像处理技术，专注于像素艺术风格的生成。

##### 应用扩展
1. 图像引导图片生成
    1. 单图像引导图片生成
        1. inpainting, 局部重绘
        2. outpainting, 扩图
    2. 多图像引导图片生成
        1. 人脸、商品数字分身
            1. 单人数字分身LoRA模型训练, 妙鸭相机
            2. DreamBooth训练单一商品模型
2. 特征注入引导图片生成
    1. ControlNet, 换动作
    2. IP-adapter, 换脸

##### 模型加速
1. 算子加速-最大化GPU利用率(TensorRT、ONNX)
2. 蒸馏加速-减少迭代步数
    1. 数据无关
        1. LCM
    2. 数据相关
        1. ADD(SD Turbo)
        2. UFO-GEN

### 3.2 3D目标生成
1. NeRF
2. DreamFusion
3. Zero-1-to-3
4. DreamCraft3D

### 3.3 视频生成
1. AnimateDiff
2. AnimateAnyone
3. Sora
    1. 视频压缩(视频编解码器): MagViT v2
    2. 分辨率自适应Transformer: NaViT
    3. 适用于视频生成的可扩展Transformer: DiT