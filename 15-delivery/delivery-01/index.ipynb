{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💡 这节课会带给你\n",
    "\n",
    "- 如何选择 GPU 和云服务厂商，追求最高性价比\n",
    "- 带你了解全球大模型\n",
    "- 使用国产大模型服务\n",
    "- 搭建 OpenAI 代理\n",
    "- 热身：基于云平台快速部署 Stable Diffusion\n",
    "- 在本地计算机运行大型模型\n",
    "    - Ollama基础\n",
    "    - 讲解Ollama API\n",
    "    - 结合RAG运行：分析 github 仓库代码\n",
    "\n",
    "开始上课！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 硬件选型\n",
    "\n",
    "当我们为模型训练及推理做硬件选型时，NVIDIA 几乎是唯一选择。\n",
    "\n",
    "这是一家全球知名的图形处理器（GPU）公司，成立于 1993 年。\n",
    "\n",
    "因为在 GPU 领域，尤其 AI 领域芯片的垄断性优势，其创始人黄仁勋被坊间称为「黄教主」。\n",
    "\n",
    "![huangrenxun](./img/huangrenxun.webp)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 什么是 GPU？\n",
    "\n",
    "Graphical Processing Units (GPUs)\n",
    "\n",
    "- 图形处理单元(GPU)是一种功能强大的电子芯片，用于在沉浸式视频游戏、电影和其他视觉媒体中呈现丰富的 2D/3D 图形和动画\n",
    "- 因其超越 CPU 的并行矩阵运算性能，所以也被广泛应用于人工智能相关的各种系统，包括机器视觉、NLP、语音识别、自动驾驶等\n",
    "\n",
    "![nvidia gpu](./img/nvidia-gpu.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA 核心和 Tensor 核心\n",
    "\n",
    "**CUDA 核心**：\n",
    "\n",
    "- 是 NVIDIA 开发的并行计算平台和编程模型，用于 GPU 上的通用计算，就像是万能工人，可以做很多不同的工作\n",
    "- 适合游戏和图形渲染、天气预测、电影特效等场景\n",
    "\n",
    "**Tensor 核心**：\n",
    "\n",
    "- 中文叫：张量核心\n",
    "- 专门设计用于深度学习中的矩阵运算，加速深度学习算法中的关键计算过程\n",
    "- 适合语音助手、人脸识别等场景\n",
    "\n",
    "案例 1：视频渲染\n",
    "当一个电影制片公司决定制作一部具有高度视觉效果的 3D 电影时，他们需要大量的计算能力来渲染每一帧。这里，CUDA 核心非常有用，因为它们能够处理大量的细节，如光线追踪、纹理和阴影。例如，当一束光从一个光源反射到一个物体上，然后反射到摄像机上，CUDA 核心可以用来计算这个光线路径上的所有细节，确保最终的图像看起来真实并且美观。\n",
    "\n",
    "案例 2：面部识别\n",
    "安全系统、智能手机和许多应用程序现在都使用面部识别技术。这需要通过深度学习模型来识别人的面部特征。Tensor 核心在这里发挥关键作用，它们可以迅速地处理神经网络中的大量矩阵乘法和加法，确保面部识别既准确又快速。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI 领域常用 GPU\n",
    "\n",
    "这个表格依据价格进行排序，价格从低到高。\n",
    "| 显卡 | 目标市场 | 性能 | 应用场景 | 价格 |\n",
    "| :---: | :-----------: | :----: | :--------------------------------: | :----------: |\n",
    "| T4 | 企业/AI 推理 | 适中 | AI 推理, 轻量级训练, 图形渲染 | 7999(14G) |\n",
    "| 4090 | 消费者 | 非常高 | 通用计算, 图形渲染, 高端游戏, 4K/8K 视频编辑 | 14599(24G) |\n",
    "| A10 | 企业/图形 | 适中 | 图形渲染, 轻量级计算 | 18999(24G) |\n",
    "| A6000 | 企业/图形 | 适中 | 图形渲染, 轻量级计算 | 32999（48G） |\n",
    "| V100 | 数据中心/AI | 高 | 深度学习训练/推理, 高性能计算 | 42999(32G) |\n",
    "| A100 | 数据中心/AI | 高 | 深度学习训练/推理, 高性能计算 | 69999(40G) |\n",
    "| A800 | 数据中心/AI | 中等 | 深度学习推理, 高性能计算, 大数据分析 | 110000 |\n",
    "| H100 | 数据中心/AI | 高 | 深度学习训练/推理, 高性能计算, 大数据分析 | 242000 |\n",
    "\n",
    "- 有些在京东就能买到：https://item.jd.com/10065826100148.html\n",
    "- 美国商务部限制 GPU 对华出口的算力不超过 4800 TOPS 和带宽不超过 600 GB/s，导致最强的 H100 和 A100 禁售。黄教主随后推出针对中国市场的 A800 和 H800。\n",
    "\n",
    "参考：\n",
    "\n",
    "- [英伟达 A100 和 H100 已被禁止向中国供货](https://www.sec.gov/ix?doc=/Archives/edgar/data/1045810/000104581022000146/nvda-20220826.htm)\n",
    "- [50 亿美元，算力芯片迎来狂欢，腾讯字节抢购英伟达 A800 订单](https://www.sohu.com/a/710949877_120746727)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H100 与 A100：H100 比 A100 快多少？\n",
    "\n",
    "16-bit 推理快约 3.5 倍，16-bit 训练快约 2.3 倍。\n",
    "\n",
    "![a100-h100-a](./img/a100-h100-a.png)\n",
    "\n",
    "<!-- ![a100-h100-b](./img/a100-h100-b.png) -->\n",
    "<!-- ![a100-h100-c](./img/a100-h100-c.png) -->\n",
    "\n",
    "参考资料：https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 什么是 LPU™ 推理引擎？\n",
    "\n",
    "LPU 推理引擎，其中 LPU 代表语言处理单元，是一种新型的端到端处理单元 ™ 系统，它为计算密集型应用程序提供最快的推理，并具有顺序组件，例如 AI 语言应用程序 （LLMs）。\n",
    "\n",
    "<img src=\"./img/groq.png\" width=\"600px\">\n",
    "\n",
    "## 谁创造了 LPU™?\n",
    "\n",
    "Jonathan Ross - 前谷歌工程师,参与设计 TPU 芯片核心。后创办 Groq 公司,创造了世界首个语言处理单元 LPU™。\n",
    "\n",
    "LPU™ 推理引擎可提供卓越 AI 工作负载速度,比其他领先供应商快 18 倍。\n",
    "\n",
    "Ross 曾尝试在谷歌将 TPU 作为 20%项目开发。后来创办 Groq,打造\"端到端\"LPU 生态系统,推出 GroqCloud/GroqRack/GroqNode/GroqCard 等产品。\n",
    "\n",
    "### Groq 硬件产品\n",
    "\n",
    "| 产品       | 描述                                                              | 规模/性能                                                                  | 功耗                |\n",
    "| ---------- | ----------------------------------------------------------------- | -------------------------------------------------------------------------- | ------------------- |\n",
    "| GroqCloud™ | 由扩展的语言处理单元网络提供支持,运行速度比其他领先提供商快 18 倍 | 云服务                                                                     | -                   |\n",
    "| GroqRack™  | 低延迟、大规模部署的主干                                          | 42U 机架,最多 64 个互连芯片,端到端延迟 1.6μs,近乎线性多服务器/机架可扩展性 | 最大 35kW           |\n",
    "| GroqNode™  | 前所未有的低延迟与不折不扣的可扩展性                              | 4U 机架,8 个互连 GroqCard™ 加速器                                          | 最大 4kW            |\n",
    "| GroqCard™  | 单芯片 PCIe 卡,保证低延迟,方便服务器集成                          | 单芯片                                                                     | 最大 375W,平均 240W |\n",
    "\n",
    "### 使用云端服务\n",
    "\n",
    "[使用文档](https://console.groq.com/docs/quickstart)\n",
    "\n",
    "#### 已经支持的模型\n",
    "\n",
    "## **支持的模型**\n",
    "\n",
    "GroqCloud 目前支持以下模型:\n",
    "\n",
    "### **LLaMA2-70b**\n",
    "\n",
    "- **开发者:** Meta\n",
    "- **模型名称:** LLaMA2-70b-chat\n",
    "- **上下文窗口:** 4,096 个 token\n",
    "- **API 字符串:** `llama2-70b-4096`\n",
    "\n",
    "### **Mixtral-8x7b**\n",
    "\n",
    "- **开发者:** Mistral\n",
    "- **模型名称:** Mixtral-8x7b-Instruct-v0.1\n",
    "- **上下文窗口:** 32,768 个 token\n",
    "- **API 字符串:** `mixtral-8x7b-32768`\n",
    "\n",
    "这些都是对话类型的模型,可以直接通过使用各自的 API 字符串在 GroqCloud 模型 API 端点访问。\n",
    "\n",
    "### Graq 性能\n",
    "\n",
    "延迟：发送 API 请求后收到第一个令牌块的时间（以秒为单位）\n",
    "\n",
    "<img src=\"./img/groq-latency.png\" width=\"500\">\n",
    "\n",
    "吞吐量：模型生成令牌时每秒收到的令牌数（即从 API 收到第一个块后）\n",
    "\n",
    "<img src=\"./img/groq-throuhput.png\" width=\"500\">\n",
    "\n",
    "越低越好，绿色代表最具吸引力的象限\n",
    "\n",
    "<img src=\"./img/groq-total-response.png\" width=\"500\">\n",
    "\n",
    "### 相关资料\n",
    "\n",
    "[为什么是 GROQ](https://wow.groq.com/why-groq/)\n",
    "\n",
    "[关于 GROQ](https://wow.groq.com/about-us/)\n",
    "[GROQ 性能测试分析](https://wow.groq.com/artificialanalysis-ai-llm-benchmark-doubles-axis-to-fit-new-groq-lpu-inference-engine-performance-results/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 物理机 vs. 云服务\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>划重点：</b>\n",
    "<ul>\n",
    "<li>如果经常做微调实验，有自己的物理机会方便很多很多</li>\n",
    "<li>提供推理服务，首选云服务</li>\n",
    "<li>如果有自建机房或 IDC，请随意</li>\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **云服务厂商对比**\n",
    "\n",
    "### 国内主流\n",
    "\n",
    "- **阿里云**：https://www.aliyun.com/product/ecs/gpu （可[申请免费试用](https://free.aliyun.com/?product=9602825&spm=5176.28055625.J_5831864660.9.e939154aYoM8ST&scm=20140722.M_9553144.P_154.MO_1802-ID_9553144-MID_9553144-CID_20080-ST_7663-V_1)）\n",
    "- **腾讯云**：https://cloud.tencent.com/act/pro/gpu-study\n",
    "- **火山引擎**：https://www.volcengine.com/product/gpu\n",
    "\n",
    "### 国外主流\n",
    "\n",
    "- **AWS**：[https://aws.amazon.com](https://aws.amazon.com)\n",
    "- **Vultr**：[https://www.vultr.com](https://www.vultr.com)\n",
    "- **TPU**：[https://cloud.google.com/tpu](https://cloud.google.com/tpu?hl=zh-cn)\n",
    "\n",
    "TPU 是 Google 专门用于加速机器学习的硬件。它特别适合大规模深度学习任务，通过高效的架构在性能和能源消耗上表现出色。\n",
    "\n",
    "它的优点和应用场景：\n",
    "\n",
    "1. **高性能和能效：** TPU 可以更快地完成任务，同时消耗较少的能源，降低成本。\n",
    "\n",
    "2. **大规模训练：** TPU 适用于大规模深度学习训练，能够高效地处理大量数据。\n",
    "\n",
    "3. **实时推理：** 适合需要快速响应的任务，如实时图像识别和文本分析。\n",
    "\n",
    "4. **云端使用：** Google Cloud 提供 TPU 服务，允许用户根据需求使用，无需购买硬件。\n",
    "\n",
    "适用于图像处理、自然语言处理、推荐系统等多个领域。\n",
    "\n",
    "在国外，科研机构、大公司和初创企业普遍使用 TPU。\n",
    "\n",
    "#### 下面是对两款 NVIDIA GPU 在他主流厂商的价格进行对比：\n",
    "\n",
    "- A100：在云服务中，A100 是顶级的企业级 GPU，适用于高性能计算需求。\n",
    "- T4：相比之下，T4 更为经济，适合日常模型微调和推理任务。\n",
    "\n",
    "NVIDIA A100：\n",
    "\n",
    "| 云服务提供商 | GPU 型号 | CPU 核心数 | 内存（GiB） | 价格（元/小时） |\n",
    "| ------------ | -------- | ---------- | ----------- | --------------- |\n",
    "| 火山引擎     | A100     | 14 核      | 245         | 40.39           |\n",
    "| 阿里云       | A100     | 16 vCPU    | 125         | 34.742          |\n",
    "| 腾讯云       | A100     | 16 核      | 96          | 28.64           |\n",
    "\n",
    "NVIDIA T4：\n",
    "\n",
    "| 云服务提供商 | CPU 核心数 | 内存（GiB） | GPU 型号 | 价格（元/小时） |\n",
    "| ------------ | ---------- | ----------- | -------- | --------------- |\n",
    "| 阿里云       | 4 vCPU     | 15          | T4       | 11.63           |\n",
    "| 火山引擎     | 4 核       | 16          | T4       | 11.28           |\n",
    "| 腾讯云       | 8 核       | 32          | T4       | 8.68            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **算力平台**\n",
    "\n",
    "主要用于学习和训练，不适合提供服务。\n",
    "\n",
    "- **Colab**：谷歌出品，升级服务仅需 9 美金。https://colab.google.com\n",
    "- **Kaggle**：免费，每周 30 小时 T4，P100 可用。https://www.kaggle.com\n",
    "- **AutoDL**：价格亲民，支持 Jupyter Notebook 及 ssh，国内首选。https://www.autodl.com\n",
    "\n",
    "**建议**：若需高速下载，尤其依赖于 GitHub 或 Docker 官方镜像，建议选择国外服务器。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 根据场景选择 GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是我们为您提供的，基于显卡 4090 上的 chatglm 和 chatglm2 模型的 Fine tuning 实验数据概览：\n",
    "\n",
    "| 模型     | 数据条数 | 时长    | 技术 |\n",
    "| -------- | -------- | ------- | ---- |\n",
    "| chatglm  | 9999     | 1:42:46 | pt2  |\n",
    "| chatglm  | 39333    | 6:45:21 | pt2  |\n",
    "| chatglm  | 9999     | 1:31:05 | Lora |\n",
    "| chatglm  | 39333    | 5:40:16 | Lora |\n",
    "| chatglm2 | 9999     | 1:50:27 | pt2  |\n",
    "| chatglm2 | 39333    | 7:26:25 | pt2  |\n",
    "| chatglm2 | 9999     | 1:29:08 | Lora |\n",
    "| chatglm2 | 39333    | 5:45:08 | Lora |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 下面是 llm-utils 上一些选型的建议\n",
    "\n",
    "- Falcon 是目前为止 huggingface 上排行榜第一的模型\n",
    "\n",
    "根据不同的使用情境，以下是使用的建议 GPU：\n",
    "\n",
    "| 用例                             | 显卡要求                                                    | 推荐显卡                |\n",
    "| -------------------------------- | ----------------------------------------------------------- | ----------------------- |\n",
    "| Running Falcon-40B               | 运行 Falcon-40B 所需的显卡应该有 85GB 到 100GB 或更多的显存 | See Falcon-40B table    |\n",
    "| Running MPT-30B                  | 当运行 MPT-30B 时，显卡应该具有 80GB 的显存                 | See MPT-30B table       |\n",
    "| Training LLaMA (65B)             | 对于训练 LLaMA (65B)，使用 8000 台 Nvidia A100 显卡。       | Very large H100 cluster |\n",
    "| Training Falcon (40B)            | 训练 Falcon (40B) 需要 384 台具有 40GB 显存的 A100 显卡。   | Large H100 cluster      |\n",
    "| Fine tuning an LLM (large scale) | 大规模微调 LLM 需要 64 台 40GB 显存的 A100 显卡             | H100 cluster            |\n",
    "| Fine tuning an LLM (small scale) | 小规模微调 LLM 则需要 4 台 80GB 显存的 A100 显卡。          | Multi-H100 instance     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>划重点：</b>\n",
    "<ul>\n",
    "<li>对于本地个人研发项目，GeForce RTX 4090 等消费级 GPU 足以满足中等规模的需求。</li>\n",
    "<li>对于公司的大规模数据和复杂模型，推荐使用如 NVIDIA A100 的高性能 GPU。</li>\n",
    "<li>数据规模小时，可考虑预算内的 A10 或 T4 型号。</li>\n",
    "<li>如果追求性价比，可以选择把 4090 显卡搭建服务器使用，也可以选择市面的第三方服务，比如：AutoDL 的 4090 服务</li>\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考资料：\n",
    "\n",
    "- https://gpus.llm-utils.org/cloud-gpu-guide/\n",
    "- https://gpus.llm-utils.org/nvidia-h100-gpus-supply-and-demand/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 服务器价格计算器\n",
    "\n",
    "火山引擎提供的这个价格计算器很方便，做个大概的云服务器 GPU 选型价格参考。其它服务厂商价格相差不是很多。\n",
    "\n",
    "https://www.volcengine.com/pricing?product=ECS&tab=2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全球大模型\n",
    "\n",
    "<!-- ### 全球大模型列表 -->\n",
    "\n",
    "<!-- <img src=\"./img/open-compass.png\" width=\"600px\"> -->\n",
    "\n",
    "### 大模型综合排名\n",
    "\n",
    "[chat-lmsys](https://chat.lmsys.org/)\n",
    "\n",
    "![ranking.png](./img/ranking.png)\n",
    "\n",
    "\n",
    "### 参考资料：\n",
    "\n",
    "[llmmodels](https://llmmodels.org/)\n",
    "\n",
    "[opencompass](https://rank.opencompass.org.cn/leaderboard-llm-v2)\n",
    "\n",
    "[huggingface models](https://huggingface.co/models)\n",
    "\n",
    "[llm.extractum](https://llm.extractum.io/list/?trending)\n",
    "\n",
    "## 模型分析\n",
    "\n",
    "![artificial-analysis](./img/artificial-analysis.png)\n",
    "\n",
    "![quality-comparison.png](./img/quality-comparison.png)\n",
    "\n",
    "\n",
    "### 按照智商排名\n",
    "\n",
    "[ais-ranked-by-iq-ai](https://www.maximumtruth.org/p/ais-ranked-by-iq-ai-passes-100-iq)\n",
    "![ranked-iq](./img/ranked-iq.png)\n",
    "\n",
    "### 总结：\n",
    "\n",
    "OpenAI 的 GPT-4 在质量指标方面是明显的质量领导者。然而，包括 Gemini Pro 和 Mixtral 8x7B 在内的型号在某些方面已经达到了 GPT-3.5 的性能。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 搭建 OpenAI 代理\n",
    "\n",
    "### 业务流程\n",
    "\n",
    "因为业务在国内，所以大部分业务代码在国内的服务器，国内服务器调用代理服务器即可。\n",
    "\n",
    "#### 注意\n",
    "\n",
    "- 不建议为了调用 openai 把业务代码放在国外，多少会有些延迟。\n",
    "\n",
    "### 根基：透传\n",
    "\n",
    "#### 实现方案\n",
    "\n",
    "##### nginx 方案\n",
    "\n",
    "```nginx\n",
    "server\n",
    "{\n",
    "    listen 80;\n",
    "    server_name a.openaixx.com;\n",
    "    index index.html;\n",
    "    location / {\n",
    "            proxy_pass https://api.openai.com;\n",
    "            proxy_ssl_name api.openai.com;\n",
    "            proxy_ssl_server_name on;\n",
    "            proxy_set_header Host api.openai.com;\n",
    "            proxy_set_header Upgrade $http_upgrade;\n",
    "            proxy_set_header Connection 'upgrade';\n",
    "            chunked_transfer_encoding off;\n",
    "            proxy_read_timeout 3600;\n",
    "            proxy_buffering off;\n",
    "            proxy_cache off;\n",
    "            proxy_redirect off;\n",
    "            proxy_hide_header Cache-Control;\n",
    "    }\n",
    "\n",
    "    location ~ /.well-known {\n",
    "        allow all;\n",
    "    }\n",
    "\n",
    "    access_log off;\n",
    "}\n",
    "```\n",
    "\n",
    "参考：\n",
    "[proxy-nginx](https://github.com/agicto/agi-proxy/blob/master/nginx/proxy.conf)\n",
    "\n",
    "#### node 服务\n",
    "\n",
    "[agi-proxy](https://github.com/agicto/agi-proxy)\n",
    "\n",
    "#### 纯 js 方案\n",
    "\n",
    "复制以下代码，去 cloudflare 建立一个 worker 即可\n",
    "\n",
    "[worker](https://github.com/agicto/agi-proxy/blob/master/worker/index.js)\n",
    "\n",
    "#### 其它语言方案，可以参考以上的思路来实现\n",
    "\n",
    "#### 服务器选择\n",
    "\n",
    "| 服务商                | 访问地址                                         | 主要服务                | 特性                                               | 适用场景                             | 起始价格             |\n",
    "| --------------------- | ------------------------------------------------ | ----------------------- | -------------------------------------------------- | ------------------------------------ | -------------------- |\n",
    "| Cloudflare            | [cloudflare.com](https://cloudflare.com/)        | CDN, 安全服务           | 全球 CDN, DDoS 保护, 自动 HTTPS                    | 增强网站性能和安全                   | 免费计划开始         |\n",
    "| Vercel                | [vercel.com](https://vercel.com/)                | 静态站点和 SSR 应用托管 | 集成 Git, 自动部署, 无服务器函数                   | 前端开发和 JAMstack 项目             | 免费计划开始         |\n",
    "| Render                | [render.com](https://render.com)                 | 应用托管, 数据库托管    | 易于使用, 自动部署, 免费 SSL 证书                  | 适合所有类型的 Web 应用和数据库托管  | 免费计划开始         |\n",
    "| DigitalOcean          | [digitalocean.com](https://digitalocean.com)     | 云基础设施服务          | 简单易用, SSD 存储, 数据中心选择                   | 中小型企业的 Web 应用托管            | $5/月起              |\n",
    "| AWS                   | aws.amazon.com                                   | 综合云服务              | 广泛的服务选择, 可扩展性, 全球数据中心             | 适合各种规模和需求的企业             | 按使用付费，有免费层 |\n",
    "| Microsoft Azure       | azure.microsoft.com                              | 综合云服务              | 多样的服务, 企业级功能, 混合云支持                 | 企业级应用和混合云解决方案           | 按使用付费，有免费层 |\n",
    "| Google Cloud Platform | cloud.google.com                                 | 综合云服务              | 高性能计算服务, 数据分析, 机器学习                 | 数据密集型应用和机器学习项目         | 按使用付费，有免费层 |\n",
    "| 阿里云国际版          | [intl.aliyun.com](https://www.alibabacloud.com/) | 综合云服务              | 全球化数据中心, 多语言客户支持, 丰富的云产品和服务 | 适合需要在中国以外地区扩展业务的企业 | 按使用付费           |\n",
    "\n",
    "#### 选择合适的域名运营商\n",
    "\n",
    "建议域名在国外的域名服务商，DNS 解析路径缩短\n",
    "\n",
    "#### 测试速度\n",
    "\n",
    "搭建完代理后，在国内的云服务器测试\n",
    "\n",
    "![ping-run.png.png](./img/ping-run.png)\n",
    "\n",
    "![own-build.png](./img/own-build.png)\n",
    "\n",
    "#### 建议方案\n",
    "\n",
    "建议方案国外提供 CDN 云服务商结合自建云服务业务做负载均衡\n",
    "\n",
    "### 加入业务\n",
    "\n",
    "- 限制模型\n",
    "- 限制接口\n",
    "- 限制速率\n",
    "\n",
    "### 实战案例\n",
    "\n",
    "利用 vercel 部署一个自己的 OpenAI 代理\n",
    "\n",
    "### 推荐项目：\n",
    "\n",
    "[agi-proxy](https://github.com/agicto/agi-proxy)\n",
    "\n",
    "[openai-forward](https://github.com/KenyonY/openai-forward)\n",
    "\n",
    "[one-api](https://github.com/songquanpeng/one-api)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 国产大模型介绍\n",
    "\n",
    "### 国产大模型列表\n",
    "\n",
    "[国产模型列表](https://github.com/wgwang/awesome-LLMs-In-China)\n",
    "\n",
    "| 公司     | 名称         | 网址                                                                                                                     | 备注                                                                                                                                                                                          |\n",
    "| -------- | ------------ | ------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| 百度     | 文心一言     | [https://yiyan.baidu.com/](https://yiyan.baidu.com/)                                                                     |                                                                                                                                                                                               |\n",
    "| 阿里云   | 通义千问     | [https://tongyi.aliyun.com/](https://tongyi.aliyun.com/)                                                                 | 开源模型[Qwen-1.8B,7B,14B,72B](https://github.com/QwenLM/Qwen)、[Qwen-VL](https://github.com/QwenLM/Qwen-VL)和[Qwen-Audio](https://github.com/QwenLM/Qwen-Audio)                              |\n",
    "| 科大讯飞 | 星火         | [https://xinghuo.xfyun.cn/](https://xinghuo.xfyun.cn/)                                                                   |                                                                                                                                                                                               |\n",
    "| 百川智能 | 百川         | [https://chat.baichuan-ai.com/](https://chat.baichuan-ai.com/)                                                           | 开源小模型[baichuan-7B](https://github.com/baichuan-inc/baichuan-7B)和[Baichuan-13B](https://github.com/baichuan-inc/Baichuan-13B)                                                            |\n",
    "| 零一万物 | Yi           | [https://github.com/01-ai/Yi](https://github.com/01-ai/Yi)                                                               | 6B 和 34B 开源模型                                                                                                                                                                            |\n",
    "| 360      | 智脑/一见    | [https://ai.360.cn/](https://ai.360.cn/), [https://github.com/360CVGroup/SEEChat](https://github.com/360CVGroup/SEEChat) |                                                                                                                                                                                               |\n",
    "| 昆仑万维 | 天工 Skywork | [https://github.com/SkyworkAI/Skywork](https://github.com/SkyworkAI/Skywork)                                             | 开源且可商用，无需单独申请，Skywork 是由昆仑万维集团·天工团队开发的一系列大型模型，本次开源的模型有 Skywork-13B-Base 模型、Skywork-13B-Chat 模型、Skywork-13B-Math 模型和 Skywork-13B-MM 模型 |\n",
    "| 腾讯     | 混元         | [https://hunyuan.tencent.com/](https://hunyuan.tencent.com/)                                                             |                                                                                                                                                                                               |\n",
    "| 月之暗面 | Moonshot     | [https://www.moonshot.cn/](https://www.moonshot.cn/)                                                                     | “长文本”大模型 支持 20 万字输入                                                                                                                                                               |\n",
    "| 商汤科技 | 商量         | [https://chat.sensetime.com/](https://chat.sensetime.com/)                                                               |                                                                                                                                                                                               |\n",
    "\n",
    "### 文心一言 API 接入指南\n",
    "\n",
    "https://cloud.baidu.com/article/1089328\n",
    "\n",
    "![yiyan-use](./img/yiyan-use.png)\n",
    "\n",
    "1. 创建千帆应用。根据实际需求创建千帆应用，创建成功后，获取 AppID、API Key、Secret Key 等信息。如果已有千帆应用，可以查看已有应用的 API Key、Secret Key 等信息。\n",
    "2. API 授权。应用创建成功后，千帆平台默认为应用开通所有 API 调用权限，无需申请授权。\n",
    "3. 获取接口访问凭证 access_token。根据第 1 步获取的 API Key 和 Secret Key ，调用获取 access_token 接口获取 access_token ，通过 access_token 鉴权调用者身份。\n",
    "4. 调用 API 接口。例如调用 ERNIE-Bot 相关接口，详见[API 列表](https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Nlks5zkzu)。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在本地计算机运行大模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>考虑到 cpp 方案的兼容性</b>\n",
    "<ol>\n",
    "<li>在Windows下兼容性比较差</li>\n",
    "</ol>\n",
    "</div>\n",
    "\n",
    "\n",
    "#### llms cpp 实验方案地址\n",
    "\n",
    "[chat-glm-cpp](./cpp.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ollama\n",
    "\n",
    "支持的模型：\n",
    "\n",
    "几乎所有的热门模型都支持\n",
    "\n",
    "https://ollama.com/library\n",
    "\n",
    "https://ollama.com/\n",
    "\n",
    "### OpenWebUI\n",
    "\n",
    "[openwebui](https://openwebui.com/)\n",
    "\n",
    "![OpenWebUI-1.png](./img/OpenWebUI-1.png)\n",
    "\n",
    "\n",
    "### lobe chat ui\n",
    "\n",
    "[lobe-chat-ui 文档地址](https://lobehub.com/docs/usage/features/local-llm)\n",
    "\n",
    "\n",
    "```\n",
    "docker run -d -p 3210:3210 -e OLLAMA_PROXY_URL=http://host.docker.internal:11434/v1 lobehub/lobe-chat\n",
    "```\n",
    "\n",
    "### 封装 API\n",
    "\n",
    "```\n",
    "curl http://localhost:11434/v1/chat/completions \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "        \"model\": \"wizardlm2\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"你是谁？\"\n",
    "            }\n",
    "        ]\n",
    "    }'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其它开源平台\n",
    "\n",
    "## llmstudio 平台\n",
    "\n",
    "https://lmstudio.ai/\n",
    "\n",
    "https://github.com/li-plus/chatglm.cpp\n",
    "\n",
    "https://github.com/ggerganov/llama.cpp\n",
    "\n",
    "## 开源 Chat UI\n",
    "\n",
    "[lobe-chat](https://github.com/lobehub/lobe-chat)\n",
    "\n",
    "[ChatGPT-Next-Web](https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web)\n",
    "\n",
    "[open-webui](https://github.com/open-webui/open-webui)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- #### 什么是 TorchServe？\n",
    "\n",
    "TorchServe 是一个专为 PyTorch 设计的模型服务工具，它可以帮助开发者轻松地部署、管理和提供 PyTorch 模型的服务。\n",
    "它是由 PyTorch 团队与亚马逊 AWS 团队共同开发的，旨在为 PyTorch 用户提供一个简单、灵活且高效的模型部署解决方案。\n",
    "\n",
    "### TorchServe 是一个强大的工具，提供了多项功能和优势：\n",
    "\n",
    "- 模型管理 API：通过优化工作角色与模型的分配，实现多模型管理。\n",
    "- 推理 API：支持 REST 和 gRPC 的批量推理。\n",
    "- 性能指南：内置支持优化、基准测试和分析 PyTorch 和 TorchServe 的性能。\n",
    "- 富有表现力的处理程序架构：通过多种开箱即用的支持，轻松支持各种用例的推理。\n",
    "\n",
    "![torchserve](./img/torchserve.png)\n",
    "\n",
    "### 模型的打包与部署\n",
    "\n",
    "详细参考实验：[TorchServe 实验](./lab/TorchServe-lab.ipynb) -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作业\n",
    "\n",
    "- 注册一个国产模型平台，并且按照 api 文档成功调用 chat completions 接口，并且接入 Web UI。\n",
    "- 本地成功运行一个大模型，并且结合 任何一个 web UI 可以顺利使用。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
