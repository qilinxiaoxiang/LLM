{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ğŸ’¡ è¿™èŠ‚è¯¾ä¼šå¸¦ç»™ä½ \n",
    "\n",
    "1. å¦‚ä½•ä½¿ç”¨ LangChainï¼šä¸€å¥—åœ¨å¤§æ¨¡å‹èƒ½åŠ›ä¸Šå°è£…çš„å·¥å…·æ¡†æ¶\n",
    "2. å¦‚ä½•ç”¨å‡ è¡Œä»£ç å®ç°ä¸€ä¸ªå¤æ‚çš„ AI åº”ç”¨\n",
    "3. é¢å‘å¤§æ¨¡å‹çš„æµç¨‹å¼€å‘çš„è¿‡ç¨‹æŠ½è±¡\n",
    "\n",
    "å¼€å§‹ä¸Šè¯¾ï¼\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ è¿™èŠ‚è¯¾æ€ä¹ˆå­¦\n",
    "\n",
    "ä»£ç èƒ½åŠ›è¦æ±‚ï¼š**ä¸­é«˜**ï¼ŒAI/æ•°å­¦åŸºç¡€è¦æ±‚ï¼š**æ— **\n",
    "\n",
    "1. æœ‰ç¼–ç¨‹åŸºç¡€çš„åŒå­¦\n",
    "   - å…³æ³¨è®¾è®¡æ€è·¯ï¼Œå®ç°ç»†èŠ‚\n",
    "2. æ²¡æœ‰ç¼–ç¨‹åŸºç¡€çš„åŒå­¦\n",
    "   - å°½é‡ç†è§£ SDK çš„æ¦‚å¿µå’Œä»·å€¼ï¼Œå°è¯•ä½“ä¼šä½¿ç”¨ SDK å‰åçš„å·®åˆ«ä¸æ„ä¹‰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å†™åœ¨å‰é¢\n",
    "\n",
    "- LangChain ä¹Ÿæ˜¯ä¸€å¥—é¢å‘å¤§æ¨¡å‹çš„å¼€å‘æ¡†æ¶ï¼ˆSDKï¼‰\n",
    "- LangChain æ˜¯ AGI æ—¶ä»£è½¯ä»¶å·¥ç¨‹çš„ä¸€ä¸ªæ¢ç´¢å’ŒåŸå‹\n",
    "- LangChain è¿­ä»£é€Ÿåº¦æ˜æ˜¾å¿«äº Semantic Kernelï¼Œå‡ ä¹æ¯å¤©ä¸€ä¸ªç‰ˆæœ¬\n",
    "- å­¦ä¹  Langchain è¦å…³æ³¨æ¥å£å˜æ›´\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>æ¸©é¦¨æç¤ºï¼š</b>\n",
    "<ol>\n",
    "<li>å®éªŒå®¤å¹³å°å·²ç»å†…ç½®äº†æœ¬è¯¾ä»¶ä¾èµ–æ‰€æœ‰çš„åŒ…ï¼Œç›¸å…³ä¸‹è½½åŒ…å‘½ä»¤å·²ç»æ³¨é‡Šï¼Œå¦‚æœåœ¨æœ¬åœ°è¿è¡Œç›¸å…³ä»£ç ï¼Œåˆ™éœ€è¦å®‰è£…æ‰€éœ€ä¾èµ–åŒ…</li>\n",
    "<li>å®éªŒå®¤å¹³å°ä¸æ”¯æŒ gpt-4 æ¨¡å‹ï¼Œå¦‚æœéœ€è¦ä½“éªŒ gpt-4 æ•ˆæœï¼Œè¯·å‚è€ƒAGIè¯¾å ‚æ‰‹å†Œï¼š https://a.agiclass.ai </li>\n",
    "</ol>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain vs. Semantic Kernel\n",
    "\n",
    "[![Star History Chart](https://api.star-history.com/svg?repos=langchain-ai/langchain,microsoft/semantic-kernel,langchain-ai/langchainjs&type=Date)](https://star-history.com/#langchain-ai/langchain&microsoft/semantic-kernel&langchain-ai/langchainjs&Date)\n",
    "\n",
    "æ•°æ®æ¥æºï¼šhttps://star-history.com/#langchain-ai/langchain&microsoft/semantic-kernel&langchain-ai/langchainjs&Date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain çš„æ ¸å¿ƒç»„ä»¶\n",
    "\n",
    "1. æ¨¡å‹ I/O å°è£…\n",
    "   - LLMsï¼šå¤§è¯­è¨€æ¨¡å‹\n",
    "   - Chat Modelsï¼šä¸€èˆ¬åŸºäº LLMsï¼Œä½†æŒ‰å¯¹è¯ç»“æ„é‡æ–°å°è£…\n",
    "   - PromptTempleï¼šæç¤ºè¯æ¨¡æ¿\n",
    "   - OutputParserï¼šè§£æè¾“å‡º\n",
    "2. æ•°æ®è¿æ¥å°è£…\n",
    "   - Document Loadersï¼šå„ç§æ ¼å¼æ–‡ä»¶çš„åŠ è½½å™¨\n",
    "   - Document Transformersï¼šå¯¹æ–‡æ¡£çš„å¸¸ç”¨æ“ä½œï¼Œå¦‚ï¼šsplit, filter, translate, extract metadata, etc\n",
    "   - Text Embedding Modelsï¼šæ–‡æœ¬å‘é‡åŒ–è¡¨ç¤ºï¼Œç”¨äºæ£€ç´¢ç­‰æ“ä½œï¼ˆå•¥æ„æ€ï¼Ÿåˆ«æ€¥ï¼Œåé¢è¯¦ç»†è®²ï¼‰\n",
    "   - Verctorstores: ï¼ˆé¢å‘æ£€ç´¢çš„ï¼‰å‘é‡çš„å­˜å‚¨\n",
    "   - Retrievers: å‘é‡çš„æ£€ç´¢\n",
    "3. è®°å¿†å°è£…\n",
    "   - Memoryï¼šè¿™é‡Œä¸æ˜¯ç‰©ç†å†…å­˜ï¼Œä»æ–‡æœ¬çš„è§’åº¦ï¼Œå¯ä»¥ç†è§£ä¸ºâ€œä¸Šæ–‡â€ã€â€œå†å²è®°å½•â€æˆ–è€…è¯´â€œè®°å¿†åŠ›â€çš„ç®¡ç†\n",
    "4. æ¶æ„å°è£…\n",
    "   - Chainï¼šå®ç°ä¸€ä¸ªåŠŸèƒ½æˆ–è€…ä¸€ç³»åˆ—é¡ºåºåŠŸèƒ½ç»„åˆ\n",
    "   - Agentï¼šæ ¹æ®ç”¨æˆ·è¾“å…¥ï¼Œè‡ªåŠ¨è§„åˆ’æ‰§è¡Œæ­¥éª¤ï¼Œè‡ªåŠ¨é€‰æ‹©æ¯æ­¥éœ€è¦çš„å·¥å…·ï¼Œæœ€ç»ˆå®Œæˆç”¨æˆ·æŒ‡å®šçš„åŠŸèƒ½\n",
    "     - Toolsï¼šè°ƒç”¨å¤–éƒ¨åŠŸèƒ½çš„å‡½æ•°ï¼Œä¾‹å¦‚ï¼šè°ƒ google æœç´¢ã€æ–‡ä»¶ I/Oã€Linux Shell ç­‰ç­‰\n",
    "     - Toolkitsï¼šæ“ä½œæŸè½¯ä»¶çš„ä¸€ç»„å·¥å…·é›†ï¼Œä¾‹å¦‚ï¼šæ“ä½œ DBã€æ“ä½œ Gmail ç­‰ç­‰\n",
    "5. Callbacks\n",
    "\n",
    "<img src=\"langchain.png\" style=\"margin-left: 0px\" width=500px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ–‡æ¡£ï¼ˆä»¥ Python ç‰ˆä¸ºä¾‹ï¼‰\n",
    "\n",
    "- åŠŸèƒ½æ¨¡å—ï¼šhttps://python.langchain.com/docs/get_started/introduction\n",
    "- API æ–‡æ¡£ï¼šhttps://api.python.langchain.com/en/latest/langchain_api_reference.html\n",
    "- ä¸‰æ–¹ç»„ä»¶é›†æˆï¼šhttps://python.langchain.com/docs/integrations/platforms/\n",
    "- å®˜æ–¹åº”ç”¨æ¡ˆä¾‹ï¼šhttps://python.langchain.com/docs/use_cases\n",
    "- è°ƒè¯•éƒ¨ç½²ç­‰æŒ‡å¯¼ï¼šhttps://python.langchain.com/docs/guides/debugging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸€ã€æ¨¡å‹ I/O å°è£…\n",
    "\n",
    "æŠŠä¸åŒçš„æ¨¡å‹ï¼Œç»Ÿä¸€å°è£…æˆä¸€ä¸ªæ¥å£ï¼Œæ–¹ä¾¿æ›´æ¢æ¨¡å‹è€Œä¸ç”¨é‡æ„ä»£ç ã€‚\n",
    "\n",
    "### 1.1 æ¨¡å‹ APIï¼šLLM vs. ChatModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…æœ€æ–°ç‰ˆæœ¬\n",
    "#!pip install --upgrade langchain\n",
    "#!pip install --upgrade langchain-openai # v0.1.0æ–°å¢çš„åº•åŒ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸æœ¬è¯¾æ— å…³ï¼Œè¯·å¿½ç•¥\n",
    "import os\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"\"\n",
    "\n",
    "OPENAI_API_KEY = \"sk-xxxxx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 OpenAI æ¨¡å‹å°è£…\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆ‘æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œæ‚¨å¯ä»¥é—®æˆ‘é—®é¢˜æˆ–è€…å¯»æ±‚å¸®åŠ©ã€‚æ‚¨æœ‰ä»€ä¹ˆéœ€è¦æˆ‘å¸®å¿™çš„å—ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")  # é»˜è®¤æ˜¯gpt-3.5-turbo\n",
    "response = llm.invoke(\"ä½ æ˜¯è°\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 å¤šè½®å¯¹è¯ Session å°è£…\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ‚¨æ˜¯ç‹å“ç„¶å…ˆç”Ÿã€‚\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,  # ç­‰ä»·äºOpenAIæ¥å£ä¸­çš„assistant role\n",
    "    HumanMessage,  # ç­‰ä»·äºOpenAIæ¥å£ä¸­çš„user role\n",
    "    SystemMessage  # ç­‰ä»·äºOpenAIæ¥å£ä¸­çš„system role\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"ä½ æ˜¯AGIClassçš„è¯¾ç¨‹åŠ©ç†ã€‚\"),\n",
    "    HumanMessage(content=\"æˆ‘æ˜¯å­¦å‘˜ï¼Œæˆ‘å«ç‹å“ç„¶ã€‚\"),\n",
    "    AIMessage(content=\"æ¬¢è¿ï¼\"),\n",
    "    HumanMessage(content=\"æˆ‘æ˜¯è°\")\n",
    "]\n",
    "\n",
    "ret = llm.invoke(messages)\n",
    "\n",
    "print(ret.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>åˆ’é‡ç‚¹ï¼š</b>é€šè¿‡æ¨¡å‹å°è£…ï¼Œå®ç°ä¸åŒæ¨¡å‹çš„ç»Ÿä¸€æ¥å£è°ƒç”¨\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 æ¢ä¸ªå›½äº§æ¨¡å‹\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install qianfan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] [05-05 06:49:48] base.py:624 [t:140107148785472]: This key `stop` does not seem to be a parameter that the model `ERNIE-Bot-turbo` will accept\n",
      "[INFO] [05-05 06:49:48] oauth.py:222 [t:140107148785472]: trying to refresh access_token for ak `cuTPS7***`\n",
      "[INFO] [05-05 06:49:48] oauth.py:237 [t:140107148785472]: sucessfully refresh access_token\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ‚¨å¥½ï¼Œæˆ‘æ˜¯ç™¾åº¦ç ”å‘çš„çŸ¥è¯†å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ï¼Œä¸­æ–‡åæ˜¯æ–‡å¿ƒä¸€è¨€ï¼Œè‹±æ–‡åæ˜¯ERNIE Botã€‚æˆ‘èƒ½å¤Ÿä¸äººå¯¹è¯äº’åŠ¨ï¼Œå›ç­”é—®é¢˜ï¼ŒååŠ©åˆ›ä½œï¼Œé«˜æ•ˆä¾¿æ·åœ°å¸®åŠ©äººä»¬è·å–ä¿¡æ¯ã€çŸ¥è¯†å’Œçµæ„Ÿã€‚\n"
     ]
    }
   ],
   "source": [
    "# å…¶å®ƒæ¨¡å‹åˆ†è£…åœ¨ langchain_community åº•åŒ…ä¸­\n",
    "from langchain_community.chat_models import QianfanChatEndpoint\n",
    "from langchain_core.messages import HumanMessage\n",
    "import os\n",
    "\n",
    "llm = QianfanChatEndpoint(\n",
    "    qianfan_ak=os.getenv('ERNIE_CLIENT_ID'),\n",
    "    qianfan_sk=os.getenv('ERNIE_CLIENT_SECRET')\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(content=\"ä½ æ˜¯è°\")\n",
    "]\n",
    "\n",
    "ret = llm.invoke(messages)\n",
    "\n",
    "print(ret.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 æ¨¡å‹çš„è¾“å…¥ä¸è¾“å‡º\n",
    "\n",
    "<img src=\"model_io.jpg\" style=\"margin-left: 0px\" width=500px>\n",
    "\n",
    "### 1.2.1 Prompt æ¨¡æ¿å°è£…\n",
    "\n",
    "1. PromptTemplate å¯ä»¥åœ¨æ¨¡æ¿ä¸­è‡ªå®šä¹‰å˜é‡\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Template===\n",
      "input_variables=['subject'] template='ç»™æˆ‘è®²ä¸ªå…³äº{subject}çš„ç¬‘è¯'\n",
      "===Prompt===\n",
      "ç»™æˆ‘è®²ä¸ªå…³äºå°æ˜çš„ç¬‘è¯\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"ç»™æˆ‘è®²ä¸ªå…³äº{subject}çš„ç¬‘è¯\")\n",
    "print(\"===Template===\")\n",
    "print(template)\n",
    "print(\"===Prompt===\")\n",
    "print(template.format(subject='å°æ˜'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ChatPromptTemplate ç”¨æ¨¡æ¿è¡¨ç¤ºçš„å¯¹è¯ä¸Šä¸‹æ–‡\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆ‘æ˜¯AGIè¯¾å ‚çš„å®¢æœåŠ©æ‰‹ï¼Œæˆ‘çš„åå­—å«ç“œç“œã€‚æœ‰ä»€ä¹ˆé—®é¢˜æˆ‘å¯ä»¥å¸®åŠ©ä½ è§£å†³å‘¢ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"ä½ æ˜¯{product}çš„å®¢æœåŠ©æ‰‹ã€‚ä½ çš„åå­—å«{name}\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "prompt = template.format_messages(\n",
    "    product=\"AGIè¯¾å ‚\",\n",
    "    name=\"ç“œç“œ\",\n",
    "    query=\"ä½ æ˜¯è°\"\n",
    ")\n",
    "\n",
    "ret = llm.invoke(prompt)\n",
    "\n",
    "print(ret.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. MessagesPlaceholder æŠŠå¤šè½®å¯¹è¯å˜æˆæ¨¡æ¿\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "\n",
    "human_prompt = \"Translate your answer to {language}.\"\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    # variable_name æ˜¯ message placeholder åœ¨æ¨¡æ¿ä¸­çš„å˜é‡å\n",
    "    # ç”¨äºåœ¨èµ‹å€¼æ—¶ä½¿ç”¨\n",
    "    [MessagesPlaceholder(variable_name=\"conversation\"), human_message_template]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Who is Elon Musk?'), AIMessage(content='Elon Musk is a billionaire entrepreneur, inventor, and industrial designer'), HumanMessage(content='Translate your answer to æ—¥æ–‡.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "human_message = HumanMessage(content=\"Who is Elon Musk?\")\n",
    "ai_message = AIMessage(\n",
    "    content=\"Elon Musk is a billionaire entrepreneur, inventor, and industrial designer\"\n",
    ")\n",
    "\n",
    "messages = chat_prompt.format_prompt(\n",
    "    # å¯¹ \"conversation\" å’Œ \"language\" èµ‹å€¼\n",
    "    conversation=[human_message, ai_message], language=\"æ—¥æ–‡\"\n",
    ")\n",
    "\n",
    "print(messages.to_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ã‚¤ãƒ¼ãƒ­ãƒ³ãƒ»ãƒã‚¹ã‚¯ã¯å„„ä¸‡é•·è€…ã®èµ·æ¥­å®¶ã€ç™ºæ˜å®¶ã€ç”£æ¥­ãƒ‡ã‚¶ã‚¤ãƒŠãƒ¼ã§ã™ã€‚\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(messages)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>åˆ’é‡ç‚¹ï¼š</b>æŠŠPromptæ¨¡æ¿çœ‹ä½œå¸¦æœ‰å‚æ•°çš„å‡½æ•°ï¼Œå¯ç±»æ¯”äº SK çš„ Semantic Function\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2ã€ä»æ–‡ä»¶åŠ è½½ Prompt æ¨¡æ¿\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Template===\n",
      "input_variables=['topic'] template='ä¸¾ä¸€ä¸ªå…³äº{topic}çš„ä¾‹å­'\n",
      "===Prompt===\n",
      "ä¸¾ä¸€ä¸ªå…³äºé»‘è‰²å¹½é»˜çš„ä¾‹å­\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_file(\"example_prompt_template.txt\")\n",
    "print(\"===Template===\")\n",
    "print(template)\n",
    "print(\"===Prompt===\")\n",
    "print(template.format(topic='é»‘è‰²å¹½é»˜'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 è¾“å‡ºå°è£… OutputParser\n",
    "\n",
    "è‡ªåŠ¨æŠŠ LLM è¾“å‡ºçš„å­—ç¬¦ä¸²æŒ‰æŒ‡å®šæ ¼å¼åŠ è½½ã€‚\n",
    "\n",
    "LangChain å†…ç½®çš„ OutputParser åŒ…æ‹¬:\n",
    "\n",
    "- ListParser\n",
    "- DatetimeParser\n",
    "- EnumParser\n",
    "- JsonOutputParser\n",
    "- PydanticParser\n",
    "- XMLParser\n",
    "\n",
    "ç­‰ç­‰\n",
    "\n",
    "### 1.3.1 Pydantic (JSON) Parser\n",
    "\n",
    "è‡ªåŠ¨æ ¹æ® Pydantic ç±»çš„å®šä¹‰ï¼Œç”Ÿæˆè¾“å‡ºçš„æ ¼å¼è¯´æ˜\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from typing import List, Dict\n",
    "\n",
    "# å®šä¹‰ä½ çš„è¾“å‡ºå¯¹è±¡\n",
    "\n",
    "\n",
    "class Date(BaseModel):\n",
    "    year: int = Field(description=\"Year\")\n",
    "    month: int = Field(description=\"Month\")\n",
    "    day: int = Field(description=\"Day\")\n",
    "    era: str = Field(description=\"BC or AD\")\n",
    "\n",
    "    # ----- å¯é€‰æœºåˆ¶ --------\n",
    "    # ä½ å¯ä»¥æ·»åŠ è‡ªå®šä¹‰çš„æ ¡éªŒæœºåˆ¶\n",
    "    @validator('month')\n",
    "    def valid_month(cls, field):\n",
    "        if field <= 0 or field > 12:\n",
    "            raise ValueError(\"æœˆä»½å¿…é¡»åœ¨1-12ä¹‹é—´\")\n",
    "        return field\n",
    "\n",
    "    @validator('day')\n",
    "    def valid_day(cls, field):\n",
    "        if field <= 0 or field > 31:\n",
    "            raise ValueError(\"æ—¥æœŸå¿…é¡»åœ¨1-31æ—¥ä¹‹é—´\")\n",
    "        return field\n",
    "\n",
    "    @validator('day', pre=True, always=True)\n",
    "    def valid_date(cls, day, values):\n",
    "        year = values.get('year')\n",
    "        month = values.get('month')\n",
    "\n",
    "        # ç¡®ä¿å¹´ä»½å’Œæœˆä»½éƒ½å·²ç»æä¾›\n",
    "        if year is None or month is None:\n",
    "            return day  # æ— æ³•éªŒè¯æ—¥æœŸï¼Œå› ä¸ºæ²¡æœ‰å¹´ä»½å’Œæœˆä»½\n",
    "\n",
    "        # æ£€æŸ¥æ—¥æœŸæ˜¯å¦æœ‰æ•ˆ\n",
    "        if month == 2:\n",
    "            if cls.is_leap_year(year) and day > 29:\n",
    "                raise ValueError(\"é—°å¹´2æœˆæœ€å¤šæœ‰29å¤©\")\n",
    "            elif not cls.is_leap_year(year) and day > 28:\n",
    "                raise ValueError(\"éé—°å¹´2æœˆæœ€å¤šæœ‰28å¤©\")\n",
    "        elif month in [4, 6, 9, 11] and day > 30:\n",
    "            raise ValueError(f\"{month}æœˆæœ€å¤šæœ‰30å¤©\")\n",
    "\n",
    "        return day\n",
    "\n",
    "    @staticmethod\n",
    "    def is_leap_year(year):\n",
    "        if year % 400 == 0 or (year % 4 == 0 and year % 100 != 0):\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Format Instruction=====\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"year\": {\"title\": \"Year\", \"description\": \"Year\", \"type\": \"integer\"}, \"month\": {\"title\": \"Month\", \"description\": \"Month\", \"type\": \"integer\"}, \"day\": {\"title\": \"Day\", \"description\": \"Day\", \"type\": \"integer\"}, \"era\": {\"title\": \"Era\", \"description\": \"BC or AD\", \"type\": \"string\"}}, \"required\": [\"year\", \"month\", \"day\", \"era\"]}\n",
      "```\n",
      "====Prompt=====\n",
      "æå–ç”¨æˆ·è¾“å…¥ä¸­çš„æ—¥æœŸã€‚\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"year\": {\"title\": \"Year\", \"description\": \"Year\", \"type\": \"integer\"}, \"month\": {\"title\": \"Month\", \"description\": \"Month\", \"type\": \"integer\"}, \"day\": {\"title\": \"Day\", \"description\": \"Day\", \"type\": \"integer\"}, \"era\": {\"title\": \"Era\", \"description\": \"BC or AD\", \"type\": \"string\"}}, \"required\": [\"year\", \"month\", \"day\", \"era\"]}\n",
      "```\n",
      "ç”¨æˆ·è¾“å…¥:\n",
      "2023å¹´å››æœˆ6æ—¥å¤©æ°”æ™´...\n",
      "====æ¨¡å‹åŸå§‹è¾“å‡º=====\n",
      "{\n",
      "  \"year\": 2023,\n",
      "  \"month\": 4,\n",
      "  \"day\": 6,\n",
      "  \"era\": \"AD\"\n",
      "}\n",
      "====Parseåçš„è¾“å‡º=====\n",
      "year=2023 month=4 day=6 era='AD'\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "\n",
    "model_name = 'gpt-3.5-turbo'\n",
    "temperature = 0\n",
    "model = ChatOpenAI(model_name=model_name, temperature=temperature)\n",
    "\n",
    "# æ ¹æ®Pydanticå¯¹è±¡çš„å®šä¹‰ï¼Œæ„é€ ä¸€ä¸ªOutputParser\n",
    "parser = PydanticOutputParser(pydantic_object=Date)\n",
    "\n",
    "template = \"\"\"æå–ç”¨æˆ·è¾“å…¥ä¸­çš„æ—¥æœŸã€‚\n",
    "{format_instructions}\n",
    "ç”¨æˆ·è¾“å…¥:\n",
    "{query}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"query\"],\n",
    "    # ç›´æ¥ä»OutputParserä¸­è·å–è¾“å‡ºæè¿°ï¼Œå¹¶å¯¹æ¨¡æ¿çš„å˜é‡é¢„å…ˆèµ‹å€¼\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "print(\"====Format Instruction=====\")\n",
    "print(parser.get_format_instructions())\n",
    "\n",
    "\n",
    "query = \"2023å¹´å››æœˆ6æ—¥å¤©æ°”æ™´...\"\n",
    "model_input = prompt.format_prompt(query=query)\n",
    "\n",
    "print(\"====Prompt=====\")\n",
    "print(model_input.to_string())\n",
    "\n",
    "output = model.invoke(model_input.to_messages())\n",
    "print(\"====æ¨¡å‹åŸå§‹è¾“å‡º=====\")\n",
    "print(output.content)\n",
    "print(\"====Parseåçš„è¾“å‡º=====\")\n",
    "date = parser.parse(output.content)\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Auto-Fixing Parser\n",
    "\n",
    "åˆ©ç”¨ LLM è‡ªåŠ¨æ ¹æ®è§£æå¼‚å¸¸ä¿®å¤å¹¶é‡æ–°è§£æ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===æ ¼å¼é”™è¯¯çš„Output===\n",
      "{\n",
      "  \"year\": 2023,\n",
      "  \"month\": å››æœˆ,\n",
      "  \"day\": 6,\n",
      "  \"era\": \"AD\"\n",
      "}\n",
      "===å‡ºç°å¼‚å¸¸===\n",
      "Invalid json output: {\n",
      "  \"year\": 2023,\n",
      "  \"month\": å››æœˆ,\n",
      "  \"day\": 6,\n",
      "  \"era\": \"AD\"\n",
      "}\n",
      "===é‡æ–°è§£æç»“æœ===\n",
      "{\"year\": 2023, \"month\": 4, \"day\": 6, \"era\": \"AD\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import OutputFixingParser\n",
    "\n",
    "new_parser = OutputFixingParser.from_llm(\n",
    "    parser=parser, llm=ChatOpenAI(model=\"gpt-3.5-turbo\"))\n",
    "\n",
    "# æˆ‘ä»¬æŠŠä¹‹å‰outputçš„æ ¼å¼æ”¹é”™\n",
    "output = output.content.replace(\"4\", \"å››æœˆ\")\n",
    "print(\"===æ ¼å¼é”™è¯¯çš„Output===\")\n",
    "print(output)\n",
    "try:\n",
    "    date = parser.parse(output)\n",
    "except Exception as e:\n",
    "    print(\"===å‡ºç°å¼‚å¸¸===\")\n",
    "    print(e)\n",
    "\n",
    "# ç”¨OutputFixingParserè‡ªåŠ¨ä¿®å¤å¹¶è§£æ\n",
    "date = new_parser.parse(output)\n",
    "print(\"===é‡æ–°è§£æç»“æœ===\")\n",
    "print(date.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>æ€è€ƒï¼š</b>çŒœä¸€ä¸‹OutputFixingParseræ˜¯æ€ä¹ˆåšåˆ°çš„\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4ã€å°ç»“\n",
    "\n",
    "1. LangChain ç»Ÿä¸€å°è£…äº†å„ç§æ¨¡å‹çš„è°ƒç”¨æ¥å£ï¼ŒåŒ…æ‹¬è¡¥å…¨å‹å’Œå¯¹è¯å‹ä¸¤ç§\n",
    "2. LangChain æä¾›äº† PromptTemplate ç±»ï¼Œå¯ä»¥è‡ªå®šä¹‰å¸¦å˜é‡çš„æ¨¡æ¿\n",
    "3. LangChain æä¾›äº†ä¸€äº›åˆ—è¾“å‡ºè§£æå™¨ï¼Œç”¨äºå°†å¤§æ¨¡å‹çš„è¾“å‡ºè§£ææˆç»“æ„åŒ–å¯¹è±¡ï¼›é¢å¤–å¸¦æœ‰è‡ªåŠ¨ä¿®å¤åŠŸèƒ½ã€‚\n",
    "4. ä¸Šè¿°æ¨¡å‹å±äº LangChain ä¸­è¾ƒä¸ºä¼˜ç§€çš„éƒ¨åˆ†ï¼›ç¾ä¸­ä¸è¶³çš„æ˜¯ OutputParser è‡ªèº«çš„ Prompt ç»´æŠ¤åœ¨ä»£ç ä¸­ï¼Œè€¦åˆåº¦è¾ƒé«˜ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## äºŒã€æ•°æ®è¿æ¥å°è£…\n",
    "\n",
    "<img src=\"data_connection.jpg\" style=\"margin-left: 0px\" width=500px>\n",
    "\n",
    "### 2.1 æ–‡æ¡£åŠ è½½å™¨ï¼šDocument Loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2 : Open Foundation and Fine-Tuned Chat Models\n",
      "Hugo Touvronâˆ—Louis Martinâ€ Kevin Stoneâ€ \n",
      "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
      "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
      "Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\n",
      "Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\n",
      "Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\n",
      "Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\n",
      "Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\n",
      "Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\n",
      "Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
      "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
      "Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n",
      "Sergey Edunov Thomas Scialomâˆ—\n",
      "GenAI, Meta\n",
      "Abstract\n",
      "In this work, we develop and release Llama 2, a collection of pretrained and ï¬ne-tuned\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "Our ï¬ne-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our\n",
      "models outperform open-source chat models on most benchmarks we tested, and based on\n",
      "our human evaluations for helpfulness and safety, may be a suitable substitute for closed-\n",
      "source models. We provide a detailed description of our approach to ï¬ne-tuning and safety\n",
      "improvements of Llama 2-Chat in order to enable the community to build on our work and\n",
      "contribute to the responsible development of LLMs.\n",
      "âˆ—Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\n",
      "â€ Second author\n",
      "Contributions for all the authors can be found in Section A.1.arXiv:2307.09288v2  [cs.CL]  19 Jul 2023\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"llama2.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "print(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 æ–‡æ¡£å¤„ç†å™¨\n",
    "\n",
    "### 2.2.1 TextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2 : Open Foundation and Fine-Tuned Chat Models\n",
      "Hugo Touvronâˆ—Louis Martinâ€ Kevin Stoneâ€ \n",
      "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
      "-------\n",
      "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
      "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
      "-------\n",
      "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
      "Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\n",
      "-------\n",
      "Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\n",
      "Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\n",
      "-------\n",
      "Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\n",
      "Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\n",
      "-------\n",
      "Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\n",
      "Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\n",
      "-------\n",
      "Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\n",
      "Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\n",
      "-------\n",
      "Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\n",
      "Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\n",
      "-------\n",
      "Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\n",
      "Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
      "-------\n",
      "Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
      "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
      "-------\n",
      "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
      "Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n",
      "Sergey Edunov Thomas Scialomâˆ—\n",
      "-------\n",
      "Sergey Edunov Thomas Scialomâˆ—\n",
      "GenAI, Meta\n",
      "Abstract\n",
      "In this work, we develop and release Llama 2, a collection of pretrained and ï¬ne-tuned\n",
      "-------\n",
      "Abstract\n",
      "In this work, we develop and release Llama 2, a collection of pretrained and ï¬ne-tuned\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "-------\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "Our ï¬ne-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our\n",
      "-------\n",
      "Our ï¬ne-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our\n",
      "models outperform open-source chat models on most benchmarks we tested, and based on\n",
      "-------\n",
      "models outperform open-source chat models on most benchmarks we tested, and based on\n",
      "our human evaluations for helpfulness and safety, may be a suitable substitute for closed-\n",
      "-------\n",
      "our human evaluations for helpfulness and safety, may be a suitable substitute for closed-\n",
      "source models. We provide a detailed description of our approach to ï¬ne-tuning and safety\n",
      "-------\n",
      "source models. We provide a detailed description of our approach to ï¬ne-tuning and safety\n",
      "improvements of Llama 2-Chat in order to enable the community to build on our work and\n",
      "-------\n",
      "improvements of Llama 2-Chat in order to enable the community to build on our work and\n",
      "contribute to the responsible development of LLMs.\n",
      "-------\n",
      "contribute to the responsible development of LLMs.\n",
      "âˆ—Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\n",
      "â€ Second author\n",
      "-------\n",
      "âˆ—Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\n",
      "â€ Second author\n",
      "Contributions for all the authors can be found in Section A.1.arXiv:2307.09288v2  [cs.CL]  19 Jul 2023\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=100, \n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "paragraphs = text_splitter.create_documents([pages[0].page_content])\n",
    "for para in paragraphs:\n",
    "    print(para.page_content)\n",
    "    print('-------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "LangChain çš„ PDFLoader å’Œ TextSplitter å®ç°éƒ½æ¯”è¾ƒç²—ç³™ï¼Œå®é™…ç”Ÿäº§ä¸­ä¸å»ºè®®ä½¿ç”¨ã€‚\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3ã€å‘é‡æ•°æ®åº“ä¸å‘é‡æ£€ç´¢\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb\n",
    "import sys\n",
    "__import__('pysqlite3')\n",
    "\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but are not releasing.Â§\n",
      "2.Llama 2-Chat , a ï¬ne-tuned version of Llama 2 that is optimized for dialogue use cases. We release\n",
      "variants of this model with 7B, 13B, and 70B parameters as well.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# åŠ è½½æ–‡æ¡£\n",
    "loader = PyPDFLoader(\"llama2.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# æ–‡æ¡£åˆ‡åˆ†\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents(\n",
    "    [page.page_content for page in pages[:4]]\n",
    ")\n",
    "\n",
    "# çŒåº“\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "# æ£€ç´¢ top-1 ç»“æœ\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "docs = retriever.get_relevant_documents(\"llama2 chatæœ‰å¤šå°‘å‚æ•°\")\n",
    "\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ›´å¤šçš„ä¸‰æ–¹æ£€ç´¢ç»„ä»¶é“¾æ¥ï¼Œå‚è€ƒï¼šhttps://python.langchain.com/docs/integrations/vectorstores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4ã€å°ç»“\n",
    "\n",
    "1. æ–‡æ¡£å¤„ç†éƒ¨åˆ† LangChain å®ç°è¾ƒä¸ºç²—ç³™ï¼Œå®é™…ç”Ÿäº§ä¸­ä¸å»ºè®®ä½¿ç”¨\n",
    "2. ä¸å‘é‡æ•°æ®åº“çš„é“¾æ¥éƒ¨åˆ†æœ¬è´¨æ˜¯æ¥å£å°è£…ï¼Œå‘é‡æ•°æ®åº“éœ€è¦è‡ªå·±é€‰å‹\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸‰ã€è®°å¿†å°è£…ï¼šMemory\n",
    "\n",
    "### 3.1ã€å¯¹è¯ä¸Šä¸‹æ–‡ï¼šConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: ä½ å¥½å•Š\\nAI: ä½ ä¹Ÿå¥½å•Š'}\n",
      "{'history': 'Human: ä½ å¥½å•Š\\nAI: ä½ ä¹Ÿå¥½å•Š\\nHuman: ä½ å†å¥½å•Š\\nAI: ä½ åˆå¥½å•Š'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
    "\n",
    "history = ConversationBufferMemory()\n",
    "history.save_context({\"input\": \"ä½ å¥½å•Š\"}, {\"output\": \"ä½ ä¹Ÿå¥½å•Š\"})\n",
    "\n",
    "print(history.load_memory_variables({}))\n",
    "\n",
    "history.save_context({\"input\": \"ä½ å†å¥½å•Š\"}, {\"output\": \"ä½ åˆå¥½å•Š\"})\n",
    "\n",
    "print(history.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2ã€åªä¿ç•™ä¸€ä¸ªçª—å£çš„ä¸Šä¸‹æ–‡ï¼šConversationBufferWindowMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: ç¬¬äºŒè½®é—®\\nAI: ç¬¬äºŒè½®ç­”\\nHuman: ç¬¬ä¸‰è½®é—®\\nAI: ç¬¬ä¸‰è½®ç­”'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "window = ConversationBufferWindowMemory(k=2)\n",
    "window.save_context({\"input\": \"ç¬¬ä¸€è½®é—®\"}, {\"output\": \"ç¬¬ä¸€è½®ç­”\"})\n",
    "window.save_context({\"input\": \"ç¬¬äºŒè½®é—®\"}, {\"output\": \"ç¬¬äºŒè½®ç­”\"})\n",
    "window.save_context({\"input\": \"ç¬¬ä¸‰è½®é—®\"}, {\"output\": \"ç¬¬ä¸‰è½®ç­”\"})\n",
    "print(window.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3ã€é€šè¿‡ Token æ•°æ§åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦ï¼šConversationTokenBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'AI: ä½ å¥½ï¼Œæˆ‘æ˜¯ä½ çš„AIåŠ©æ‰‹ã€‚\\nHuman: ä½ ä¼šå¹²ä»€ä¹ˆ\\nAI: æˆ‘ä»€ä¹ˆéƒ½ä¼š'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "memory = ConversationTokenBufferMemory(\n",
    "    llm=ChatOpenAI(),\n",
    "    max_token_limit=40\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"ä½ å¥½å•Š\"}, {\"output\": \"ä½ å¥½ï¼Œæˆ‘æ˜¯ä½ çš„AIåŠ©æ‰‹ã€‚\"})\n",
    "memory.save_context(\n",
    "    {\"input\": \"ä½ ä¼šå¹²ä»€ä¹ˆ\"}, {\"output\": \"æˆ‘ä»€ä¹ˆéƒ½ä¼š\"})\n",
    "\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4ã€æ›´å¤šç±»å‹\n",
    "\n",
    "- ConversationSummaryMemory: å¯¹ä¸Šä¸‹æ–‡åšæ‘˜è¦\n",
    "  - https://python.langchain.com/docs/modules/memory/types/summary\n",
    "- ConversationSummaryBufferMemory: ä¿å­˜ Token æ•°é™åˆ¶å†…çš„ä¸Šä¸‹æ–‡ï¼Œå¯¹æ›´æ—©çš„åšæ‘˜è¦\n",
    "  - https://python.langchain.com/docs/modules/memory/types/summary_buffer\n",
    "- VectorStoreRetrieverMemory: å°† Memory å­˜å‚¨åœ¨å‘é‡æ•°æ®åº“ä¸­ï¼Œæ ¹æ®ç”¨æˆ·è¾“å…¥æ£€ç´¢å›æœ€ç›¸å…³çš„éƒ¨åˆ†\n",
    "  - https://python.langchain.com/docs/modules/memory/types/vectorstore_retriever_memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5ã€å°ç»“\n",
    "\n",
    "1. LangChain çš„ Memory ç®¡ç†æœºåˆ¶å±äºå¯ç”¨çš„éƒ¨åˆ†ï¼Œå°¤å…¶æ˜¯ç®€å•æƒ…å†µå¦‚æŒ‰è½®æ•°æˆ–æŒ‰ Token æ•°ç®¡ç†ï¼›\n",
    "2. å¯¹äºå¤æ‚æƒ…å†µï¼Œå®ƒä¸ä¸€å®šæ˜¯æœ€ä¼˜çš„å®ç°ï¼Œä¾‹å¦‚æ£€ç´¢å‘é‡åº“æ–¹å¼ï¼Œå»ºè®®æ ¹æ®å®é™…æƒ…å†µå’Œæ•ˆæœè¯„ä¼°ï¼›\n",
    "3. ä½†æ˜¯**å®ƒå¯¹å†…å­˜çš„å„ç§ç»´æŠ¤æ–¹æ³•çš„æ€è·¯åœ¨å®é™…ç”Ÿäº§ä¸­å¯ä»¥å€Ÿé‰´**ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å››ã€Chain å’Œ LangChain Expression Language (LCEL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain Expression Languageï¼ˆLCELï¼‰æ˜¯ä¸€ç§å£°æ˜å¼è¯­è¨€ï¼Œå¯è½»æ¾ç»„åˆä¸åŒçš„è°ƒç”¨é¡ºåºæ„æˆ Chainã€‚LCEL è‡ªåˆ›ç«‹ä¹‹åˆå°±è¢«è®¾è®¡ä¸ºèƒ½å¤Ÿæ”¯æŒå°†åŸå‹æŠ•å…¥ç”Ÿäº§ç¯å¢ƒï¼Œ**æ— éœ€ä»£ç æ›´æ”¹**ï¼Œä»æœ€ç®€å•çš„â€œæç¤º+LLMâ€é“¾åˆ°æœ€å¤æ‚çš„é“¾ï¼ˆå·²æœ‰ç”¨æˆ·æˆåŠŸåœ¨ç”Ÿäº§ç¯å¢ƒä¸­è¿è¡ŒåŒ…å«æ•°ç™¾ä¸ªæ­¥éª¤çš„ LCEL Chainï¼‰ã€‚\n",
    "\n",
    "LCEL çš„ä¸€äº›äº®ç‚¹åŒ…æ‹¬ï¼š\n",
    "\n",
    "1. **æµæ”¯æŒ**ï¼šä½¿ç”¨ LCEL æ„å»º Chain æ—¶ï¼Œä½ å¯ä»¥è·å¾—æœ€ä½³çš„é¦–ä¸ªä»¤ç‰Œæ—¶é—´ï¼ˆå³ä»è¾“å‡ºå¼€å§‹åˆ°é¦–æ‰¹è¾“å‡ºç”Ÿæˆçš„æ—¶é—´ï¼‰ã€‚å¯¹äºæŸäº› Chainï¼Œè¿™æ„å‘³ç€å¯ä»¥ç›´æ¥ä» LLM æµå¼ä¼ è¾“ä»¤ç‰Œåˆ°æµè¾“å‡ºè§£æå™¨ï¼Œä»è€Œä»¥ä¸ LLM æä¾›å•†è¾“å‡ºåŸå§‹ä»¤ç‰Œç›¸åŒçš„é€Ÿç‡è·å¾—è§£æåçš„ã€å¢é‡çš„è¾“å‡ºã€‚\n",
    "\n",
    "2. **å¼‚æ­¥æ”¯æŒ**ï¼šä»»ä½•ä½¿ç”¨ LCEL æ„å»ºçš„é“¾æ¡éƒ½å¯ä»¥é€šè¿‡åŒæ­¥ APIï¼ˆä¾‹å¦‚ï¼Œåœ¨ Jupyter ç¬”è®°æœ¬ä¸­è¿›è¡ŒåŸå‹è®¾è®¡æ—¶ï¼‰å’Œå¼‚æ­¥ APIï¼ˆä¾‹å¦‚ï¼Œåœ¨ LangServe æœåŠ¡å™¨ä¸­ï¼‰è°ƒç”¨ã€‚è¿™ä½¿å¾—ç›¸åŒçš„ä»£ç å¯ç”¨äºåŸå‹è®¾è®¡å’Œç”Ÿäº§ç¯å¢ƒï¼Œå…·æœ‰å‡ºè‰²çš„æ€§èƒ½ï¼Œå¹¶èƒ½å¤Ÿåœ¨åŒä¸€æœåŠ¡å™¨ä¸­å¤„ç†å¤šä¸ªå¹¶å‘è¯·æ±‚ã€‚\n",
    "\n",
    "3. **ä¼˜åŒ–çš„å¹¶è¡Œæ‰§è¡Œ**ï¼šå½“ä½ çš„ LCEL é“¾æ¡æœ‰å¯ä»¥å¹¶è¡Œæ‰§è¡Œçš„æ­¥éª¤æ—¶ï¼ˆä¾‹å¦‚ï¼Œä»å¤šä¸ªæ£€ç´¢å™¨ä¸­è·å–æ–‡æ¡£ï¼‰ï¼Œæˆ‘ä»¬ä¼šè‡ªåŠ¨æ‰§è¡Œï¼Œæ— è®ºæ˜¯åœ¨åŒæ­¥è¿˜æ˜¯å¼‚æ­¥æ¥å£ä¸­ï¼Œä»¥å®ç°æœ€å°çš„å»¶è¿Ÿã€‚\n",
    "\n",
    "4. **é‡è¯•å’Œå›é€€**ï¼šä¸º LCEL é“¾çš„ä»»ä½•éƒ¨åˆ†é…ç½®é‡è¯•å’Œå›é€€ã€‚è¿™æ˜¯ä½¿é“¾åœ¨è§„æ¨¡ä¸Šæ›´å¯é çš„ç»ä½³æ–¹å¼ã€‚ç›®å‰æˆ‘ä»¬æ­£åœ¨æ·»åŠ é‡è¯•/å›é€€çš„æµåª’ä½“æ”¯æŒï¼Œå› æ­¤ä½ å¯ä»¥åœ¨ä¸å¢åŠ ä»»ä½•å»¶è¿Ÿæˆæœ¬çš„æƒ…å†µä¸‹è·å¾—å¢åŠ çš„å¯é æ€§ã€‚\n",
    "\n",
    "5. **è®¿é—®ä¸­é—´ç»“æœ**ï¼šå¯¹äºæ›´å¤æ‚çš„é“¾æ¡ï¼Œè®¿é—®åœ¨æœ€ç»ˆè¾“å‡ºäº§ç”Ÿä¹‹å‰çš„ä¸­é—´æ­¥éª¤çš„ç»“æœé€šå¸¸éå¸¸æœ‰ç”¨ã€‚è¿™å¯ä»¥ç”¨äºè®©æœ€ç»ˆç”¨æˆ·çŸ¥é“æ­£åœ¨å‘ç”Ÿä¸€äº›äº‹æƒ…ï¼Œç”šè‡³ä»…ç”¨äºè°ƒè¯•é“¾æ¡ã€‚ä½ å¯ä»¥æµå¼ä¼ è¾“ä¸­é—´ç»“æœï¼Œå¹¶ä¸”åœ¨æ¯ä¸ª LangServe æœåŠ¡å™¨ä¸Šéƒ½å¯ç”¨ã€‚\n",
    "\n",
    "6. **è¾“å…¥å’Œè¾“å‡ºæ¨¡å¼**ï¼šè¾“å…¥å’Œè¾“å‡ºæ¨¡å¼ä¸ºæ¯ä¸ª LCEL é“¾æä¾›äº†ä»é“¾çš„ç»“æ„æ¨æ–­å‡ºçš„ Pydantic å’Œ JSONSchema æ¨¡å¼ã€‚è¿™å¯ä»¥ç”¨äºè¾“å…¥å’Œè¾“å‡ºçš„éªŒè¯ï¼Œæ˜¯ LangServe çš„ä¸€ä¸ªç»„æˆéƒ¨åˆ†ã€‚\n",
    "\n",
    "7. **æ— ç¼ LangSmith è·Ÿè¸ªé›†æˆ**ï¼šéšç€é“¾æ¡å˜å¾—è¶Šæ¥è¶Šå¤æ‚ï¼Œç†è§£æ¯ä¸€æ­¥å‘ç”Ÿäº†ä»€ä¹ˆå˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚é€šè¿‡ LCELï¼Œæ‰€æœ‰æ­¥éª¤éƒ½è‡ªåŠ¨è®°å½•åˆ° LangSmithï¼Œä»¥å®ç°æœ€å¤§çš„å¯è§‚å¯Ÿæ€§å’Œå¯è°ƒè¯•æ€§ã€‚\n",
    "\n",
    "8. **æ— ç¼ LangServe éƒ¨ç½²é›†æˆ**ï¼šä»»ä½•ä½¿ç”¨ LCEL åˆ›å»ºçš„é“¾éƒ½å¯ä»¥è½»æ¾åœ°ä½¿ç”¨ LangServe è¿›è¡Œéƒ¨ç½²ã€‚\n",
    "\n",
    "åŸæ–‡ï¼šhttps://python.langchain.com/docs/expression_language/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Pipeline å¼è°ƒç”¨ PromptTemplate, LLM å’Œ OutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from typing import List, Dict, Optional\n",
    "from enum import Enum\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"name\": \"ä¸è¶…è¿‡100å…ƒçš„æµé‡å¤§çš„å¥—é¤\",\n",
      "    \"price_lower\": 0,\n",
      "    \"price_upper\": 100,\n",
      "    \"data_lower\": 0,\n",
      "    \"data_upper\": 0,\n",
      "    \"sort_by\": \"data\",\n",
      "    \"ordering\": \"descend\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# è¾“å‡ºç»“æ„\n",
    "class SortEnum(str, Enum):\n",
    "    data = 'data'\n",
    "    price = 'price'\n",
    "\n",
    "\n",
    "class OrderingEnum(str, Enum):\n",
    "    ascend = 'ascend'\n",
    "    descend = 'descend'\n",
    "\n",
    "\n",
    "class Semantics(BaseModel):\n",
    "    name: Optional[str] = Field(description=\"æµé‡åŒ…åç§°\", default=None)\n",
    "    price_lower: Optional[int] = Field(description=\"ä»·æ ¼ä¸‹é™\", default=None)\n",
    "    price_upper: Optional[int] = Field(description=\"ä»·æ ¼ä¸Šé™\", default=None)\n",
    "    data_lower: Optional[int] = Field(description=\"æµé‡ä¸‹é™\", default=None)\n",
    "    data_upper: Optional[int] = Field(description=\"æµé‡ä¸Šé™\", default=None)\n",
    "    sort_by: Optional[SortEnum] = Field(description=\"æŒ‰ä»·æ ¼æˆ–æµé‡æ’åº\", default=None)\n",
    "    ordering: Optional[OrderingEnum] = Field(\n",
    "        description=\"å‡åºæˆ–é™åºæ’åˆ—\", default=None)\n",
    "\n",
    "\n",
    "# OutputParser\n",
    "parser = PydanticOutputParser(pydantic_object=Semantics)\n",
    "\n",
    "# Prompt æ¨¡æ¿\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"å°†ç”¨æˆ·çš„è¾“å…¥è§£ææˆJSONè¡¨ç¤ºã€‚è¾“å‡ºæ ¼å¼å¦‚ä¸‹ï¼š\\n{format_instructions}\\nä¸è¦è¾“å‡ºæœªæåŠçš„å­—æ®µã€‚\",\n",
    "        ),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ").partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "# æ¨¡å‹\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# LCEL è¡¨è¾¾å¼\n",
    "runnable = (\n",
    "    {\"text\": RunnablePassthrough()} | prompt | model | parser\n",
    ")\n",
    "\n",
    "# è¿è¡Œ\n",
    "ret = runnable.invoke(\"ä¸è¶…è¿‡100å…ƒçš„æµé‡å¤§çš„å¥—é¤æœ‰å“ªäº›\")\n",
    "print(json.dumps(ret.dict(),indent=4,ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>æ³¨æ„:</b> åœ¨å½“å‰çš„æ–‡æ¡£ä¸­ LCEL äº§ç”Ÿçš„å¯¹è±¡ï¼Œè¢«å«åš runnable æˆ– chainï¼Œç»å¸¸ä¸¤ç§å«æ³•æ··ç”¨ã€‚æœ¬è´¨å°±æ˜¯ä¸€ä¸ªè‡ªå®šä¹‰è°ƒç”¨æµç¨‹ã€‚\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>ä½¿ç”¨ LCEL çš„ä»·å€¼ï¼Œä¹Ÿå°±æ˜¯ LangChain çš„æ ¸å¿ƒä»·å€¼ã€‚</b> <br />\n",
    "å®˜æ–¹ä»ä¸åŒè§’åº¦ç»™å‡ºäº†ä¸¾ä¾‹è¯´æ˜ï¼šhttps://python.langchain.com/docs/expression_language/why\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 ç”¨ LCEL å®ç° RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# åŠ è½½æ–‡æ¡£\n",
    "loader = PyPDFLoader(\"llama2.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# æ–‡æ¡£åˆ‡åˆ†\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents(\n",
    "    [page.page_content for page in pages[:4]]\n",
    ")\n",
    "\n",
    "# çŒåº“\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "# æ£€ç´¢ top-1 ç»“æœ\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama 2æœ‰7B, 13B, å’Œ70Bå‚æ•°ã€‚'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "# Promptæ¨¡æ¿\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"question\": RunnablePassthrough(), \"context\": retriever}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"Llama 2æœ‰å¤šå°‘å‚æ•°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 é€šè¿‡ LCEL å®ç° Function Calling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(first_int: int, second_int: int) -> int:\n",
    "    \"\"\"ä¸¤ä¸ªæ•´æ•°ç›¸ä¹˜\"\"\"\n",
    "    return first_int * second_int\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(first_int: int, second_int: int) -> int:\n",
    "    \"Add two integers.\"\n",
    "    return first_int + second_int\n",
    "\n",
    "\n",
    "@tool\n",
    "def exponentiate(base: int, exponent: int) -> int:\n",
    "    \"Exponentiate the base to the exponent power.\"\n",
    "    return base**exponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.output_parsers import JsonOutputToolsParser\n",
    "\n",
    "tools = [multiply, add, exponentiate]\n",
    "# å¸¦æœ‰åˆ†æ”¯çš„ LCEL\n",
    "llm_with_tools = model.bind_tools(tools) | {\n",
    "    \"functions\": JsonOutputToolsParser(),\n",
    "    \"text\": StrOutputParser()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'functions': [{'args': {'first_int': 1024, 'second_int': 16}, 'type': 'multiply'}], 'text': ''}\n"
     ]
    }
   ],
   "source": [
    "result = llm_with_tools.invoke(\"1024çš„16å€æ˜¯å¤šå°‘\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'functions': [], 'text': 'æˆ‘æ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡å‹åŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©æ‚¨è§£å†³é—®é¢˜å’Œå›ç­”æŸ¥è¯¢ã€‚æœ‰ä»€ä¹ˆå¯ä»¥å¸®åˆ°æ‚¨çš„å—ï¼Ÿ'}\n"
     ]
    }
   ],
   "source": [
    "result = llm_with_tools.invoke(\"ä½ æ˜¯è°\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ç›´æ¥é€‰æ‹©å·¥å…·å¹¶è¿è¡Œï¼ˆé€‰ï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import (\n",
    "    Runnable,\n",
    "    RunnableLambda,\n",
    "    RunnableMap,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "\n",
    "# åç§°åˆ°å‡½æ•°çš„æ˜ å°„\n",
    "tool_map = {tool.name: tool for tool in tools}\n",
    "\n",
    "\n",
    "def call_tool(tool_invocation: dict) -> Union[str, Runnable]:\n",
    "    \"\"\"æ ¹æ®æ¨¡å‹é€‰æ‹©çš„ tool åŠ¨æ€åˆ›å»º LCEL\"\"\"\n",
    "    tool = tool_map[tool_invocation[\"type\"]]\n",
    "    return RunnablePassthrough.assign(\n",
    "        output=itemgetter(\"args\") | tool\n",
    "    )\n",
    "\n",
    "\n",
    "# .map() ä½¿ function é€ä¸€ä½œç”¨ä¸ä¸€ç»„è¾“å…¥\n",
    "call_tool_list = RunnableLambda(call_tool).map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'args': {'base': 1024, 'exponent': 2}, 'type': 'exponentiate', 'output': 1048576}]\n",
      "ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def route(response):\n",
    "    if len(response[\"functions\"]) > 0:\n",
    "        return response[\"functions\"]\n",
    "    else:\n",
    "        return response[\"text\"]\n",
    "\n",
    "\n",
    "llm_with_tools = model.bind_tools(tools) | {\n",
    "    \"functions\": JsonOutputToolsParser() | call_tool_list,\n",
    "    \"text\": StrOutputParser()\n",
    "} | RunnableLambda(route)\n",
    "\n",
    "result = llm_with_tools.invoke(\"1024çš„å¹³æ–¹æ˜¯å¤šå°‘\")\n",
    "print(result)\n",
    "\n",
    "result = llm_with_tools.invoke(\"ä½ å¥½\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "è¿™ç§å†™æ³•å¯è¯»æ€§å¤ªå·®äº†ï¼Œä¸ªäººä¸å»ºè®®ä½¿ç”¨è¿‡äºå¤æ‚çš„ LCELï¼\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 ç”¨ LCEL å®ç°å·¥å‚æ¨¡å¼ï¼ˆé€‰ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½ï¼Œæˆ‘æ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡å‹AIåŠ©æ‰‹ï¼Œè¢«å¼€å‘å’Œç”Ÿäº§ç”±OpenAIã€‚æˆ‘è¢«è®¾è®¡ç”¨æ¥å¸®åŠ©å›ç­”å„ç§é—®é¢˜ï¼Œæä¾›ä¿¡æ¯å’Œæ”¯æŒå¯¹è¯ã€‚æˆ‘å¯ä»¥è¿›è¡Œå¤šç§å¯¹è¯ï¼ŒåŒ…æ‹¬æä¾›å¨±ä¹æ€§çš„å†…å®¹ï¼Œå›ç­”é—®é¢˜ï¼Œç”šè‡³è¿›è¡Œå“²å­¦æ€§çš„è®¨è®ºã€‚å¸Œæœ›æˆ‘å¯ä»¥å¸®åŠ©åˆ°ä½ ï¼\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables.utils import ConfigurableField\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models import QianfanChatEndpoint\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import HumanMessage\n",
    "import os\n",
    "\n",
    "# æ¨¡å‹1\n",
    "ernie_model = QianfanChatEndpoint(\n",
    "    qianfan_ak=os.getenv('ERNIE_CLIENT_ID'),\n",
    "    qianfan_sk=os.getenv('ERNIE_CLIENT_SECRET')\n",
    ")\n",
    "\n",
    "# æ¨¡å‹2\n",
    "gpt_model = ChatOpenAI()\n",
    "\n",
    "\n",
    "# é€šè¿‡ configurable_alternatives æŒ‰æŒ‡å®šå­—æ®µé€‰æ‹©æ¨¡å‹\n",
    "model = gpt_model.configurable_alternatives(\n",
    "    ConfigurableField(id=\"llm\"), \n",
    "    default_key=\"gpt\", \n",
    "    ernie=ernie_model,\n",
    ")\n",
    "\n",
    "# Prompt æ¨¡æ¿\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# LCEL\n",
    "chain = (\n",
    "    {\"query\": RunnablePassthrough()} \n",
    "    | prompt\n",
    "    | model \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# è¿è¡Œæ—¶æŒ‡å®šæ¨¡å‹ \"gpt\" or \"ernie\"\n",
    "ret = chain.with_config(configurable={\"llm\": \"gpt\"}).invoke(\"ä»‹ç»ä½ è‡ªå·±ï¼ŒåŒ…æ‹¬ä½ çš„ç”Ÿäº§å•†\")\n",
    "\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ‰©å±•é˜…è¯»ï¼šä»€ä¹ˆæ˜¯[**å·¥å‚æ¨¡å¼**](https://www.runoob.com/design-pattern/factory-pattern.html)ï¼›[**è®¾è®¡æ¨¡å¼**](https://www.runoob.com/design-pattern/design-pattern-intro.html)æ¦‚è§ˆã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>æ€è€ƒï¼š</b>ä»æ¨¡å—é—´è§£ä¾èµ–è§’åº¦ï¼ŒLCELçš„æ„ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é€šè¿‡ LCELï¼Œè¿˜å¯ä»¥å®ç°\n",
    "\n",
    "1. é…ç½®è¿è¡Œæ—¶å˜é‡ï¼šhttps://python.langchain.com/docs/expression_language/how_to/configure\n",
    "2. æ•…éšœå›é€€ï¼šhttps://python.langchain.com/docs/expression_language/how_to/fallbacks\n",
    "3. å¹¶è¡Œè°ƒç”¨ï¼šhttps://python.langchain.com/docs/expression_language/how_to/map\n",
    "4. é€»è¾‘åˆ†æ”¯ï¼šhttps://python.langchain.com/docs/expression_language/how_to/routing\n",
    "5. è°ƒç”¨è‡ªå®šä¹‰æµå¼å‡½æ•°ï¼šhttps://python.langchain.com/docs/expression_language/how_to/generators\n",
    "6. é“¾æ¥å¤–éƒ¨ Memoryï¼šhttps://python.langchain.com/docs/expression_language/how_to/message_history\n",
    "\n",
    "æ›´å¤šä¾‹å­ï¼šhttps://python.langchain.com/docs/expression_language/cookbook/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## äº”ã€æ™ºèƒ½ä½“æ¶æ„ï¼šAgent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 å›å¿†ï¼šä»€ä¹ˆæ˜¯æ™ºèƒ½ä½“ï¼ˆAgentï¼‰\n",
    "\n",
    "å°†å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºä¸€ä¸ªæ¨ç†å¼•æ“ã€‚ç»™å®šä¸€ä¸ªä»»åŠ¡ï¼Œæ™ºèƒ½ä½“è‡ªåŠ¨ç”Ÿæˆå®Œæˆä»»åŠ¡æ‰€éœ€çš„æ­¥éª¤ï¼Œæ‰§è¡Œç›¸åº”åŠ¨ä½œï¼ˆä¾‹å¦‚é€‰æ‹©å¹¶è°ƒç”¨å·¥å…·ï¼‰ï¼Œç›´åˆ°ä»»åŠ¡å®Œæˆã€‚\n",
    "\n",
    "<img src=\"agent-overview.png\" style=\"margin-left: 0px\" width=500px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 å…ˆå®šä¹‰ä¸€äº›å·¥å…·ï¼šTools\n",
    "\n",
    "- å¯ä»¥æ˜¯ä¸€ä¸ªå‡½æ•°æˆ–ä¸‰æ–¹ API\n",
    "- ä¹Ÿå¯ä»¥æŠŠä¸€ä¸ª Chain æˆ–è€… Agent çš„ run()ä½œä¸ºä¸€ä¸ª Tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "from langchain.tools import Tool, tool\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "tools = [\n",
    "    Tool.from_function(\n",
    "        func=search.run,\n",
    "        name=\"Search\",\n",
    "        description=\"useful for when you need to answer questions about current events\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "éœ€è¦æ³¨å†Œ [SerpAPI](https://serpapi.com/)ï¼ˆé™é‡å…è´¹ï¼‰ï¼Œå¹¶å°† `SERPAPI_API_KEY` å†™åœ¨ç¯å¢ƒå˜é‡ä¸­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "import dateutil.parser as parser\n",
    "from datetime import date\n",
    "\n",
    "# è‡ªå®šä¹‰å·¥å…·\n",
    "\n",
    "\n",
    "@tool(\"weekday\")\n",
    "def weekday(date_str: str) -> str:\n",
    "    \"\"\"Convert date to weekday name\"\"\"\n",
    "    d = parser.parse(date_str)\n",
    "    return calendar.day_name[d.weekday()]\n",
    "\n",
    "\n",
    "tools += [weekday]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 æ™ºèƒ½ä½“ç±»å‹ï¼šReAct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ReAct.png\" style=\"margin-left: 0px\" width=500px>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "{tools}\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [{tool_names}]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "import json\n",
    "\n",
    "# ä¸‹è½½ä¸€ä¸ªç°æœ‰çš„ Prompt æ¨¡æ¿\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "print(prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mæˆ‘éœ€è¦çŸ¥é“å‘¨æ°ä¼¦çš„å‡ºç”Ÿæ—¥æœŸï¼Œç„¶åæˆ‘å¯ä»¥ä½¿ç”¨weekdayå‡½æ•°æ¥æ‰¾å‡ºé‚£å¤©æ˜¯æ˜ŸæœŸå‡ ã€‚\n",
      "Action: Search\n",
      "Action Input: å‘¨æ°ä¼¦çš„å‡ºç”Ÿæ—¥æœŸ\u001b[0m\u001b[36;1m\u001b[1;3mJanuary 18, 1979 (age 45 years), Linkou District, Taipei, Taiwan\u001b[0m\u001b[32;1m\u001b[1;3mæˆ‘ç°åœ¨çŸ¥é“å‘¨æ°ä¼¦çš„å‡ºç”Ÿæ—¥æœŸæ˜¯1979å¹´1æœˆ18æ—¥ã€‚æˆ‘å¯ä»¥ä½¿ç”¨weekdayå‡½æ•°æ¥æ‰¾å‡ºé‚£å¤©æ˜¯æ˜ŸæœŸå‡ ã€‚\n",
      "Action: weekday\n",
      "Action Input: \"1979-01-18\"\u001b[0m\u001b[33;1m\u001b[1;3mThursday\u001b[0m\u001b[32;1m\u001b[1;3mæˆ‘ç°åœ¨çŸ¥é“å‘¨æ°ä¼¦å‡ºç”Ÿé‚£å¤©æ˜¯æ˜ŸæœŸå››ã€‚\n",
      "Final Answer: å‘¨æ°ä¼¦å‡ºç”Ÿé‚£å¤©æ˜¯æ˜ŸæœŸå››ã€‚\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'å‘¨æ°ä¼¦å‡ºç”Ÿé‚£å¤©æ˜¯æ˜ŸæœŸå‡ ', 'output': 'å‘¨æ°ä¼¦å‡ºç”Ÿé‚£å¤©æ˜¯æ˜ŸæœŸå››ã€‚'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4', temperature=0)\n",
    "\n",
    "# å®šä¹‰ä¸€ä¸ª agent: éœ€è¦å¤§æ¨¡å‹ã€å·¥å…·é›†ã€å’Œ Prompt æ¨¡æ¿\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "# å®šä¹‰ä¸€ä¸ªæ‰§è¡Œå™¨ï¼šéœ€è¦ agent å¯¹è±¡ å’Œ å·¥å…·é›†\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# æ‰§è¡Œ\n",
    "agent_executor.invoke({\"input\": \"å‘¨æ°ä¼¦å‡ºç”Ÿé‚£å¤©æ˜¯æ˜ŸæœŸå‡ \"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 æ™ºèƒ½ä½“ç±»å‹ï¼šSelfAskWithSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who lived longer, Muhammad Ali or Alan Turing?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: How old was Muhammad Ali when he died?\n",
      "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
      "Follow up: How old was Alan Turing when he died?\n",
      "Intermediate answer: Alan Turing was 41 years old when he died.\n",
      "So the final answer is: Muhammad Ali\n",
      "\n",
      "Question: When was the founder of craigslist born?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who was the founder of craigslist?\n",
      "Intermediate answer: Craigslist was founded by Craig Newmark.\n",
      "Follow up: When was Craig Newmark born?\n",
      "Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
      "So the final answer is: December 6, 1952\n",
      "\n",
      "Question: Who was the maternal grandfather of George Washington?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who was the mother of George Washington?\n",
      "Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
      "Follow up: Who was the father of Mary Ball Washington?\n",
      "Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
      "So the final answer is: Joseph Ball\n",
      "\n",
      "Question: Are both the directors of Jaws and Casino Royale from the same country?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who is the director of Jaws?\n",
      "Intermediate answer: The director of Jaws is Steven Spielberg.\n",
      "Follow up: Where is Steven Spielberg from?\n",
      "Intermediate answer: The United States.\n",
      "Follow up: Who is the director of Casino Royale?\n",
      "Intermediate answer: The director of Casino Royale is Martin Campbell.\n",
      "Follow up: Where is Martin Campbell from?\n",
      "Intermediate answer: New Zealand.\n",
      "So the final answer is: No\n",
      "\n",
      "Question: {input}\n",
      "Are followup questions needed here:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "# ä¸‹è½½ä¸€ä¸ªæ¨¡æ¿\n",
    "prompt = hub.pull(\"hwchase17/self-ask-with-search\")\n",
    "\n",
    "print(prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mYes.\n",
      "Follow up: Who is å´äº¬'s wife?\u001b[0m\u001b[36;1m\u001b[1;3m['2æœˆ16æ—¥ã€‚ ä¸€å¼ å´äº¬å¦»å­è°¢æ¥ ä¸çŸ¥åå¥³æ¼”å‘˜è´¾ç²çš„åˆå½±åœ¨ç½‘ç»œä¸Šå¼•èµ·äº†çƒ­è®®ã€‚ ä¸¤ä½ç¾å¥³åŒæ¡†ï¼Œé¢œå€¼å–œäººï¼Œæ°”è´¨æ›´æ˜¯èˆ¬é…å¾—å¦‚åŒäº²å§å¦¹ä¸€èˆ¬ï¼Œè®©äººä¸ç¦èµå¹ä¸å·²ã€‚', 'ç®€ä»‹ï¼š ä½ çŸ¥é“å´äº¬å¨¶è¿‡å‡ ä¸ªè€å©†å—åªç»“äº†ä¸€æ¬¡å©šç›®å‰ä»–æœ‰ä¸¤ä¸ªå­©å­å´äº¬çš„è€å©†æ˜¯è°¢æ¥ ä»–ä»¬æ˜¯åœ¨2014å¹´ç»“çš„å©š2018å¹´ç”Ÿçš„ç¬¬äºŒä¸ªå­©å­å´è™‘1974å¹´4... ... å´äº¬çš„2ä½å‰ä»»æ›å…‰,ä¸€ä¸ªæ¯”ä¸€ä¸ª ...', '2023å¹´12æœˆ21æ—¥è°¢æ¥ åœ¨å´äº¬äº‹ä¸šä½è°·æ—¶çš„æ”¯æŒï¼Œè®©ä»–å¯¹å¥¹äº§ç”Ÿäº†æ·±æ·±çš„æ„Ÿæ¿€ä¸ä¾èµ–ã€‚ äº”ã€å´äº¬ä¸è°¢æ¥ çš„å©šå§»ç”Ÿæ´»2014å¹´... 2015å¹´5æœˆ6æ—¥å´äº¬çš„ä¸ªäººäº‹ä¸šå°±å¼€å§‹çº¢ç«äº†èµ·æ¥,è¦è¯´è°¢æ¥ æ˜¯ä¸ªâ€œæ—ºå¤«â€çš„å¦»å­,ä¸€ç‚¹å„¿ä¹Ÿä¸å‡ã€‚', 'ä¸»æ¼”ã€Œæˆ°ç‹¼ã€ç³»åˆ—é›»å½±æš´ç´…çš„ä¸­åœ‹ç”·æ˜Ÿå³äº¬ï¼Œå’Œå°9æ­²çš„å¦»å­è¬æ¥ çµå©šå¤šå¹´ä¾†ï¼Œæ„Ÿæƒ…ç”œèœœï¼Œæ˜¯åœˆå…§å…¬èªç¥ä»™çœ·ä¾¶ã€‚ è¿‘æ—¥ï¼Œæœ‰ç¶²å‹ç¿»å‡ºå…©äººæ˜”æ—¥ä¸€èµ·ä¸ŠçœŸäººç§€ç¶œè—ï¼Œå³äº¬åœ¨ç¯€ç›®ä¸­ç™¼è„¾æ°£çš„ç‰‡æ®µï¼Œä¸¦æŒ‡è²¬ä»–æš´åŠ›ã€å¤§ç”·å­ä¸»ç¾©ã€ä¸æ„›è€å©†ï¼Œé‚„å‹¸è¬æ¥ é›¢å©šã€‚ å°æ­¤ï¼Œè¬æ¥ ç™¼é•·æ–‡è¨´èªªçœŸå¯¦æ„Ÿæƒ³ã€‚', '4æœˆ3æ—¥ï¼Œå´äº¬è€å©†è°¢æ¥ åœ¨ç¤¾äº¤åª’ä½“å¹³å°ä¸Šç»™å´äº¬å‘48å²çš„ç”Ÿæ—¥ç¥ç¦ï¼Œå¼•èµ·äº†ç½‘å‹çš„çƒ­è®®ã€‚è°¢æ¥ å±•ç¤ºäº†å´äº¬å¹´è½»æ—¶çš„å¸…æ°”ç…§ç‰‡ï¼Œè°ƒçš®åœ°è¯´ï¼šâ€œæˆ‘ä¸ä¼šç»™ä½ çš„ç”Ÿæ—¥ ...', 'å¾ˆå¤šäººéƒ½è¯´å´äº¬ä¸ä¼šæµªæ¼«ï¼Œä¸çŸ¥é“æ€ä¹ˆå“„è€å©†å¼€å¿ƒï¼Œä»–æ˜¯ä¸€ä¸ªç¡¬æ±‰ï¼Œä¸æ‡‚å¾—æ€ä¹ˆæŸ”æƒ…ï¼Œè°¢æ¥ ä¸€ç›´ç»™å¤§å®¶çš„æ„Ÿè§‰éƒ½æ˜¯å¾ˆåšå¼ºçš„å¥³äººï¼Œä¸€ç›´éƒ½ç•™çŸ­å¤´å‘ï¼Œä½†æœ‰ä¸€æ¬¡è°¢æ¥ æµ ...', 'å´äº¬çš„è€å©†è°¢æ¥ å¤šå¤§å¹´é¾„ Â· æ˜æ˜Ÿå¤«å¦»ä½ éƒ½è®¤è¯†å—#æœ›ä»™è°·å¸¦ä½ å—¨@æœ›ä»™ Â· 48å²#å´äº¬åœ¨ç»“æŸå·¥ä½œåï¼Œè¿˜è¦è€å©†#è°¢æ¥ æ¥æ¥å¥¹ Â· #å´äº¬é›¶ç‚¹å‡†æ—¶ä¸ºè°¢æ¥ åº†ç”Ÿè°¢æ¥ 37å²çš„ç”Ÿæ—¥ ...', 'æ¼”å‘˜å´äº¬çš„å¦»å­æ˜¯è°¢æ¥ ã€‚ è°¢æ¥ ï¼Œ1983å¹´11æœˆ6æ—¥å‡ºç”Ÿäºå®‰å¾½çœåˆè‚¥å¸‚ï¼Œæ¯•ä¸šäºå®‰å¾½è´¢ç»å¤§å­¦ï¼Œä¸­å›½å†…åœ°å¥³ä¸»è¢­å‹æŒäººã€æ¼”å‘˜ã€‚2014å¹´1æœˆ1æ—¥å‡Œæ™¨ï¼Œæ™’å‡ºç»“å©šè¯ ...', 'ç»è¿‡ä¸¤å¹´çš„æ‹æƒ…å´äº¬å’Œè°¢æ¥ ç»ˆäºèµ°è¿›å©šå§»çš„æ®¿å ‚ã€‚ å¦‚ä»Šï¼Œä¸¤äººè‚²æœ‰ä¸¤ä¸ªå„¿å­ï¼šä¸€ä¸ªâ€œæ— æ‰€è°“â€ï¼Œä¸€ä¸ªâ€œæ— è™‘â€ï¼Œå¯ä»¥è¯´æ˜¯å¹¸ç¦ç¾æ»¡ã€‚ ä¸è¿‡å¯¹äºå´äº¬æ¥è¯´ï¼Œè™½ç„¶ç°åœ¨å·¥ä½œæ¯”ä»¥å‰æ›´å¤šäº†ï¼Œä½†æ˜¯ä¹Ÿä¼šæŠ½æ—¶é—´å›å®¶é™ªé™ªè€å©†å­©å­ã€‚ è¾…å¯¼å­©å­å­¦ä¹ ï¼Œæˆ–è€…çœ‹çœ‹æç¬‘çš„èŠ‚ç›®è¿™äº›ã€‚']\u001b[0m\u001b[32;1m\u001b[1;3mFollow up: What variety shows has è°¢æ¥  hosted?\u001b[0m\u001b[36;1m\u001b[1;3m['2023, Ace vs Ace Season 8 add. Chinese TV Show, 2023, 13 eps. (Ep. 1) (Guest). 13 ; 2023, Dang Ran Qing Chun add. Chinese TV Show, 2023, 12 eps. (Regular Member).', 'è°¢æ¥ ï¼Œ1983å¹´11æœˆ6æ—¥å‡ºç”Ÿäºå®‰å¾½çœåˆè‚¥å¸‚ï¼Œä¸­å›½å†…åœ°èŠ‚ç›®å¥³ä¸»æŒäººã€å½±è§†æ¼”å‘˜ï¼Œæ¯•ä¸šäºå®‰å¾½è´¢ç»å¤§å­¦ã€‚2005å¹´ï¼Œåœ¨â€œé­…åŠ›ä¸»æŒå¤§èµ›â€ä¸Šè·å¾—å† å†›å¹¶ç­¾çº¦å…‰çº¿ä¼ åª’ï¼Œä»è€Œå¼€å¯äº†å¥¹çš„ ...', 'ç°ä¸ºå…‰çº¿ä¼ åª’æ——ä¸‹ä¸»æ‰“èŠ‚ç›®ã€Šå¨±ä¹ç°åœºã€‹ã€ã€Šæœ€ä½³ç°åœºã€‹ã€ã€Šå½±è§†é£äº‘æ¦œã€‹å½“å®¶ä¸»æŒï¼Œæ›´æ˜¯é»‘é¾™æ±Ÿå«è§†ã€Šå¿«æ´»æ­¦æ—ã€‹ã€æ·±åœ³å«è§†ã€Šå¤§ç‰Œç”Ÿæ—¥ä¼šã€‹ã€è¾½å®å«è§†ã€Šè°æ˜¯ä¸»è§’ã€‹ç­‰èŠ‚ç›®å¥³ä¸»æŒã€‚', 'Hi Everyone, welcome to Variety show Here will provide the latest and ... Variety show. 315K. Subscribe ... å´äº¬è¡¨ç¤ºè°¢æ¥ å¥½éš¾è¿½ï¼Œè°¢æ¥ å½“åœºæ¾„æ¸…ï¼šåˆ«é€ è°£ ...', 'Hosted by. Hu Qiaohua Â· Yi Yi (backstage). Coaches. Na ... has the last 30 seconds to decide if they want the ... Variety shows. The Voice of China Â· 1 Â· 2 Â· 3 Â· 4.', \"']Follow up: What variety shows has è°¢æ¥ hosted?['TBA, Love Actually Season 3 add. Chinese TV Show, 0000, 10 eps. (Main Host). 10 ; 2023, Ace ...\", 'Hebei Province; the Theme Forum series are hosted ... Variety, Harmony in Diversityâ€, it has developed its unique orientation ... for variety shows I Am the Actor ...', 'éšç€æˆ‘å›½ä¸šä¸»å¯¹â€œå¥½å»ºç­‘â€çš„éœ€æ±‚æ—¥ç›Šå¢åŠ ï¼Œå›½. é™…å»ºç­‘å¤§å¸ˆåœ¨ä¸­å›½çš„åœŸåœ°. ä¸Šè®¾è®¡å»ºç­‘å·²æ˜¯å¸ç©ºè§æƒ¯ã€‚åœ¨æ„Ÿå¹å¤§å¸ˆå¸¦æ¥çš„å…¨æ–°ç¾.', 'å¹¿ä¸œç”µè§†ç»¼è‰º#å¨±ä¹æ²¡æœ‰åœˆç”Ÿæ´»ç¦»ä¸å¼€å¨±ä¹ï¼Œå¨±ä¹å²‚æ­¢äºå¨±ä¹åœˆã€‚æœ¬èŠ‚ç›®ä»¥â€œå¨±ä¹â€ä¸ºä¸»çº¿ï¼Œæ‰“é€ ä¸€æ¡£æ¶µç›–ç”Ÿæ´»æ–¹å¼ã€æ½®æµèµ„è®¯ã€ æ˜æ˜Ÿå…«å¦ã€æµè¡ŒéŸ³ä¹çš„å‘¨æ’­èŠ‚ç›® ...']\u001b[0m\u001b[32;1m\u001b[1;3mSo the final answer is: è°¢æ¥  has hosted shows like \"Ace vs Ace\", \"Dang Ran Qing Chun\", \"å¨±ä¹ç°åœº\", \"æœ€ä½³ç°åœº\", \"å½±è§†é£äº‘æ¦œ\", \"å¿«æ´»æ­¦æ—\", \"å¤§ç‰Œç”Ÿæ—¥ä¼š\", \"è°æ˜¯ä¸»è§’\", and \"Love Actually\".\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'å´äº¬çš„è€å©†ä¸»æŒè¿‡å“ªäº›ç»¼è‰ºèŠ‚ç›®',\n",
       " 'output': 'è°¢æ¥  has hosted shows like \"Ace vs Ace\", \"Dang Ran Qing Chun\", \"å¨±ä¹ç°åœº\", \"æœ€ä½³ç°åœº\", \"å½±è§†é£äº‘æ¦œ\", \"å¿«æ´»æ­¦æ—\", \"å¤§ç‰Œç”Ÿæ—¥ä¼š\", \"è°æ˜¯ä¸»è§’\", and \"Love Actually\".'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_self_ask_with_search_agent\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Intermediate Answer\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to ask with search.\",\n",
    "    )\n",
    "]\n",
    "\n",
    "# self_ask_with_search_agent åªèƒ½ä¼ ä¸€ä¸ªåä¸º 'Intermediate Answer' çš„ tool\n",
    "agent = create_self_ask_with_search_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)\n",
    "\n",
    "agent_executor.invoke({\"input\": \"å´äº¬çš„è€å©†ä¸»æŒè¿‡å“ªäº›ç»¼è‰ºèŠ‚ç›®\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>åˆ’é‡ç‚¹ï¼š</b>\n",
    "<ol>\n",
    "<li>ReAct æ˜¯æ¯”è¾ƒå¸¸ç”¨çš„ Planner</li>\n",
    "<li>SelfAskWithSearch æ›´é€‚åˆéœ€è¦å±‚å±‚æ¨ç†çš„åœºæ™¯ï¼ˆä¾‹å¦‚çŸ¥è¯†å›¾è°±ï¼‰</li>\n",
    "<li>Agentè½åœ°åº”ç”¨éœ€è¦æ›´å¤šç»†èŠ‚ï¼Œåé¢è¯¾ç¨‹ä¸­æˆ‘ä»¬ä¼šä¸“é—¨è®² Agent çš„å®ç°</li>\n",
    "</ol>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å…­ã€LangServe\n",
    "\n",
    "LangServe ç”¨äºå°† Chain æˆ–è€… Runnable éƒ¨ç½²æˆä¸€ä¸ª REST API æœåŠ¡ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£… LangServe\n",
    "# !pip install --upgrade \"langserve[all]\"\n",
    "\n",
    "# ä¹Ÿå¯ä»¥åªå®‰è£…ä¸€ç«¯\n",
    "# !pip install \"langserve[client]\"\n",
    "# !pip install \"langserve[server]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1ã€Server ç«¯\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python\n",
    "from fastapi import FastAPI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langserve import add_routes\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI(\n",
    "  title=\"LangChain Server\",\n",
    "  version=\"1.0\",\n",
    "  description=\"A simple api server using Langchain's Runnable interfaces\",\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_template(\"è®²ä¸€ä¸ªå…³äº{topic}çš„ç¬‘è¯\")\n",
    "add_routes(\n",
    "    app,\n",
    "    prompt | model,\n",
    "    path=\"/joke\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"localhost\", port=9999)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2ã€Client ç«¯\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import requests\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:9999/joke/invoke\",\n",
    "    json={'input': {'topic': 'å°æ˜'}}\n",
    ")\n",
    "print(response.json())\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸ƒã€LangChain.js\n",
    "\n",
    "Python ç‰ˆ LangChain çš„å§Šå¦¹é¡¹ç›®ï¼Œéƒ½æ˜¯ç”± Harrison Chase ä¸»ç†ã€‚\n",
    "\n",
    "é¡¹ç›®åœ°å€ï¼šhttps://github.com/langchain-ai/langchainjs\n",
    "\n",
    "æ–‡æ¡£åœ°å€ï¼šhttps://js.langchain.com/docs/\n",
    "\n",
    "ç‰¹è‰²ï¼š\n",
    "\n",
    "1. å¯ä»¥å’Œ Python ç‰ˆ LangChain æ— ç¼å¯¹æ¥\n",
    "\n",
    "2. æŠ½è±¡è®¾è®¡å®Œå…¨ç›¸åŒï¼Œæ¦‚å¿µä¸€ä¸€å¯¹åº”\n",
    "\n",
    "3. æ‰€æœ‰å¯¹è±¡åºåˆ—åŒ–åéƒ½èƒ½è·¨è¯­è¨€ä½¿ç”¨ï¼Œä½† API å·®åˆ«æŒºå¤§ï¼Œä¸è¿‡åœ¨åŠªåŠ›å¯¹é½\n",
    "\n",
    "æ”¯æŒç¯å¢ƒï¼š\n",
    "\n",
    "1. Node.js (ESM and CommonJS) - 18.x, 19.x, 20.x\n",
    "2. Cloudflare Workers\n",
    "3. Vercel / Next.js (Browser, Serverless and Edge functions)\n",
    "4. Supabase Edge Functions\n",
    "5. Browser\n",
    "6. Deno\n",
    "\n",
    "å®‰è£…ï¼š\n",
    "\n",
    "```\n",
    "npm install langchain\n",
    "```\n",
    "\n",
    "å½“å‰é‡ç‚¹ï¼š\n",
    "\n",
    "1. è¿½ä¸Š Python ç‰ˆçš„èƒ½åŠ›ï¼ˆç”šè‡³ä¸ºæ­¤åšäº†ä¸€ä¸ªåŸºäº gpt-3.5-turbo çš„ä»£ç ç¿»è¯‘å™¨ï¼‰\n",
    "2. ä¿æŒå…¼å®¹å°½å¯èƒ½å¤šçš„ç¯å¢ƒ\n",
    "3. å¯¹è´¨é‡å…³æ³¨ä¸å¤šï¼Œéšæ—¶é—´è‡ªç„¶èƒ½è§£å†³\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain ä¸ Semantic Kernel å¯¹æ¯”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| åŠŸèƒ½/å·¥å…·          |     LangChain      |       Semantic Kernel       |\n",
    "| ------------------ | :----------------: | :-------------------------: |\n",
    "| ç‰ˆæœ¬å·             |       0.1.17       |       python-0.9.6b1        |\n",
    "| é€‚é…çš„ LLM         |         å¤š         |        å°‘ + å¤–éƒ¨ç”Ÿæ€        |\n",
    "| Prompt å·¥å…·        |        æ”¯æŒ        |            æ”¯æŒ             |\n",
    "| Prompt å‡½æ•°åµŒå¥—    |   éœ€è¦é€šè¿‡ LCEL    |            æ”¯æŒ             |\n",
    "| Prompt æ¨¡æ¿åµŒå¥—    |        æ”¯æŒ        |           ä¸æ”¯æŒ            |\n",
    "| è¾“å‡ºè§£æå·¥å…·       |        æ”¯æŒ        |           ä¸æ”¯æŒ            |\n",
    "| ä¸Šä¸‹æ–‡ç®¡ç†å·¥å…·     |        æ”¯æŒ        | C#ç‰ˆæ”¯æŒï¼ŒPython ç‰ˆå°šæœªæ”¯æŒ |\n",
    "| å†…ç½®å·¥å…·           |   å¤šï¼Œä½†è‰¯è ä¸é½   |        å°‘ + å¤–éƒ¨ç”Ÿæ€        |\n",
    "| ä¸‰æ–¹å‘é‡æ•°æ®åº“é€‚é… |         å¤š         |        å°‘ + å¤–éƒ¨ç”Ÿæ€        |\n",
    "| æœåŠ¡éƒ¨ç½²           |     LangServe      |     ä¸ Azure è¡”æ¥æ›´ä¸æ»‘     |\n",
    "| ç®¡ç†å·¥å…·           | LangSmith/LangFuse |         Prompt Flow (*)        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ€»ç»“\n",
    "\n",
    "1. LangChain éšç€ç‰ˆæœ¬è¿­ä»£å¯ç”¨æ€§æœ‰æ˜æ˜¾æå‡\n",
    "2. ä½¿ç”¨ LangChain è¦æ³¨æ„ç»´æŠ¤è‡ªå·±çš„ Promptï¼Œå°½é‡ Prompt ä¸ä»£ç é€»è¾‘è§£ä¾èµ–\n",
    "3. å®ƒçš„å†…ç½®åŸºç¡€å·¥å…·ï¼Œå»ºè®®å……åˆ†æµ‹è¯•æ•ˆæœåå†å†³å®šæ˜¯å¦ä½¿ç”¨\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä½œä¸š\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç”¨ LangChain é‡æ„ ChatPDF çš„ä½œä¸š\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
